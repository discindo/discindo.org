[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Discindo",
    "section": "",
    "text": "Data science and data engineering solutions with R and Shiny in the Cloud"
  },
  {
    "objectID": "posts/2024-02-25-how-to-set-up-development-and-production-environments-using-aws-copilot-example-using-a-plumber-api/index.html",
    "href": "posts/2024-02-25-how-to-set-up-development-and-production-environments-using-aws-copilot-example-using-a-plumber-api/index.html",
    "title": "How to set up development and production environments using AWS Copilot: Example using a plumber API.",
    "section": "",
    "text": "In this post I am documenting step-by-step the process of deploying dev/stage/prod environments and instances of a {plumber} API on AWS AppRunner using AWS Copilot. This is an expanded follow up to a previous post on the topic."
  },
  {
    "objectID": "posts/2024-02-25-how-to-set-up-development-and-production-environments-using-aws-copilot-example-using-a-plumber-api/index.html#docker-images",
    "href": "posts/2024-02-25-how-to-set-up-development-and-production-environments-using-aws-copilot-example-using-a-plumber-api/index.html#docker-images",
    "title": "How to set up development and production environments using AWS Copilot: Example using a plumber API.",
    "section": "Docker images",
    "text": "Docker images\n\nBase image (Dockerfile_base)\nThis builds the base image, setting up the R environment and dependencies for the API. Assuming dependencies will not change often, this image can be pushed to ECR once and then used to rebuild the API image as the API evolves. If dependencies change, this image would have to be rebuilt and pushed to ECR.\ndocker build -d Dockerfile_base -t \"myapi_base\" .\nThe code below tags the image with the name provided by AWS when we create the registry for the base image. Then, it obtains AWS ECR login credentials and pushes the local image to AWS ECR. This makes it available for AWS Copilot, as it is needed when AWS Copilot builds our API service docker image.\ndocker tag myapi_base &lt;aws_account_number&gt;.dkr.ecr.&lt;aws_region&gt;.amazonaws.com/myapi_base\naws ecr get-login-password | \\\n  docker login -u AWS --password-stdin \\\n  &lt;aws_account_number&gt;.dkr.ecr.&lt;aws_region&gt;.amazonaws.com/myapi\n\n\nService image (Dockerfile)\nMake sure its FROM instruction is the base registry above\nFROM &lt;aws_account_number&gt;.dkr.ecr.&lt;aws_region&gt;.amazonaws.com/myapi_base\nRUN installr -d remotes\nRUN mkdir /build_zone\nADD . /build_zone\nWORKDIR /build_zone\nRUN R -e 'remotes::install_local(upgrade=\"never\")'\nRUN rm -rf /build_zone\nEXPOSE 5050\nCMD  [\"R\", \"-e\", \"library(myapi); run_api(port = 5050, host = '0.0.0.0')\"]"
  },
  {
    "objectID": "posts/2024-02-25-how-to-set-up-development-and-production-environments-using-aws-copilot-example-using-a-plumber-api/index.html#initialize-aws-resources",
    "href": "posts/2024-02-25-how-to-set-up-development-and-production-environments-using-aws-copilot-example-using-a-plumber-api/index.html#initialize-aws-resources",
    "title": "How to set up development and production environments using AWS Copilot: Example using a plumber API.",
    "section": "Initialize AWS resources",
    "text": "Initialize AWS resources\nInitialize the application with aws copilot\ncopilot app init myapi-api\nInitialize environments:\ncopilot env init --name dev --profile noob\ncopilot env init --name stage --profile noob\ncopilot env init --name prod --profile noob\nAt this point the deployment copilot directory will look like so:\ncopilot/\n├── environments\n│   ├── dev\n│   │   └── manifest.yml\n│   ├── prod\n│   │   └── manifest.yml\n│   └── stage\n│       └── manifest.yml\n└── .workspace"
  },
  {
    "objectID": "posts/2024-02-25-how-to-set-up-development-and-production-environments-using-aws-copilot-example-using-a-plumber-api/index.html#deploy",
    "href": "posts/2024-02-25-how-to-set-up-development-and-production-environments-using-aws-copilot-example-using-a-plumber-api/index.html#deploy",
    "title": "How to set up development and production environments using AWS Copilot: Example using a plumber API.",
    "section": "Deploy",
    "text": "Deploy\nFor each environment, copilot will first deploy the environment using CloudFromation, and then push build and push the Docker image to Elastic Container Registry, and finally configure AppRunner to make the service available.\nDev env\ncopilot init -d ./Dockerfile --app myapi-api -n myapi -t \"Request-Driven Web Service\" -e dev\nStage env\ncopilot init -d ./Dockerfile --app myapi-api -n myapi -t \"Request-Driven Web Service\" -e stage\nProd env\ncopilot init -d ./Dockerfile --app myapi-api -n myapi -t \"Request-Driven Web Service\" -e prod"
  },
  {
    "objectID": "posts/2024-02-25-how-to-set-up-development-and-production-environments-using-aws-copilot-example-using-a-plumber-api/index.html#secrets",
    "href": "posts/2024-02-25-how-to-set-up-development-and-production-environments-using-aws-copilot-example-using-a-plumber-api/index.html#secrets",
    "title": "How to set up development and production environments using AWS Copilot: Example using a plumber API.",
    "section": "Secrets",
    "text": "Secrets\nCreate the secret\ncopilot secret init\n# follow promts\nUpdate the application manifest, should look like this:\n# You can override any of the values defined above by environment.\nenvironments:\n  dev:\n    variables:\n      LOG_LEVEL: debug # Log level for the \"test\" environment.\n    secrets:\n      SECRET: /copilot/myapi-api/dev/secrets/SECRET\n  stage:\n    secrets:\n      SECRET: /copilot/myapi-api/stage/secrets/SECRET\n  prod:\n    secrets:\n      SECRET: /copilot/myapi-api/prod/secrets/SECRET\nRedeploy the service instance for each env\ncopilot svc deploy --env dev\ncopilot svc deploy --env stage\ncopilot svc deploy --env prod"
  },
  {
    "objectID": "posts/2024-02-25-how-to-set-up-development-and-production-environments-using-aws-copilot-example-using-a-plumber-api/index.html#aws-permission-policies-for-used-services",
    "href": "posts/2024-02-25-how-to-set-up-development-and-production-environments-using-aws-copilot-example-using-a-plumber-api/index.html#aws-permission-policies-for-used-services",
    "title": "How to set up development and production environments using AWS Copilot: Example using a plumber API.",
    "section": "AWS permission policies for used services",
    "text": "AWS permission policies for used services\n\nCloudFormation\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"cloudformation:DescribeStackSet\",\n                \"cloudformation:CreateStack\",\n                \"cloudformation:GetTemplate\",\n                \"cloudformation:DescribeStackSetOperation\",\n                \"cloudformation:DeleteStack\",\n                \"cloudformation:UpdateStack\",\n                \"cloudformation:DescribeStackResource\",\n                \"cloudformation:UpdateStackSet\",\n                \"cloudformation:CreateChangeSet\",\n                \"cloudformation:DescribeChangeSet\",\n                \"cloudformation:DeleteStackSet\",\n                \"cloudformation:DescribeStacks\",\n                \"cloudformation:TagResource\",\n                \"cloudformation:GetTemplateSummary\",\n                \"cloudformation:ListStackInstances\",\n                \"cloudformation:CreateStackInstances\",\n                \"cloudformation:ExecuteChangeSet\",\n                \"cloudformation:DescribeStackEvents\"\n            ],\n            \"Resource\": [\n                \"arn:aws:cloudformation:*:&lt;aws_account_number&gt;:type/resource/*\",\n                \"arn:aws:cloudformation:*:&lt;aws_account_number&gt;:stackset-target/*\",\n                \"arn:aws:cloudformation:*:&lt;aws_account_number&gt;:stackset/*:*\",\n                \"arn:aws:cloudformation:*:&lt;aws_account_number&gt;:stack/*/*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"cloudformation:CreateGeneratedTemplate\",\n                \"cloudformation:ListStacks\",\n                \"cloudformation:UpdateGeneratedTemplate\",\n                \"cloudformation:ListStackSets\",\n                \"cloudformation:DescribeGeneratedTemplate\",\n                \"cloudformation:CreateStackSet\",\n                \"cloudformation:ValidateTemplate\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\n\nECR\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ecr:BatchGetImage\",\n        \"ecr:CompleteLayerUpload\",\n        \"ecr:DescribeImages\",\n        \"ecr:TagResource\",\n        \"ecr:DescribeRepositories\",\n        \"ecr:BatchDeleteImage\",\n        \"ecr:UploadLayerPart\",\n        \"ecr:ListImages\",\n        \"ecr:InitiateLayerUpload\",\n        \"ecr:DeleteRepository\",\n        \"ecr:BatchCheckLayerAvailability\",\n        \"ecr:PutImage\"\n      ],\n      \"Resource\": \"arn:aws:ecr:*:&lt;aws_account_number&gt;:repository/*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ecr:CreateRepository\",\n        \"ecr:DescribeRegistry\",\n        \"ecr:GetAuthorizationToken\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n\n\n\nIAM\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"iam:GetRole\",\n        \"iam:UpdateAssumeRolePolicy\",\n        \"iam:ListRoleTags\",\n        \"iam:GetPolicy\",\n        \"iam:TagRole\",\n        \"iam:CreateRole\",\n        \"iam:PutRolePolicy\",\n        \"iam:PassRole\",\n        \"iam:CreateServiceLinkedRole\",\n        \"iam:ListAttachedRolePolicies\",\n        \"iam:UpdateRole\",\n        \"iam:ListPolicyTags\",\n        \"iam:ListRolePolicies\",\n        \"iam:GetRolePolicy\"\n      ],\n      \"Resource\": [\n        \"arn:aws:iam::&lt;aws_account_number&gt;:role/*\",\n        \"arn:aws:iam::&lt;aws_account_number&gt;:policy/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"iam:ListPolicies\",\n        \"iam:ListRoles\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n\n\n\nKMS\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"kms:Decrypt\",\n        \"kms:GenerateDataKey\",\n        \"kms:DescribeKey\"\n      ],\n      \"Resource\": \"arn:aws:kms:*:&lt;aws_account_number&gt;:key/*\"\n    }\n  ]\n}\n\n\n\nS3\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:PutObject\",\n        \"s3:GetObjectAcl\",\n        \"s3:GetObject\",\n        \"s3:DeleteObject\",\n        \"s3:PutObjectAcl\"\n      ],\n      \"Resource\": \"arn:aws:s3:::*/*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:PutBucketAcl\",\n        \"s3:CreateBucket\",\n        \"s3:ListBucket\",\n        \"s3:GetBucketAcl\",\n        \"s3:DeleteBucket\"\n      ],\n      \"Resource\": \"arn:aws:s3:::*\"\n    }\n  ]\n}\n\n\n\nSTS\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"sts:AssumeRole\",\n        \"sts:AssumeRoleWithWebIdentity\"\n      ],\n      \"Resource\": \"arn:aws:iam::&lt;aws_account_number&gt;:role/*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"sts:GetCallerIdentity\",\n      \"Resource\": \"*\"\n    }\n  ]\n}\n\n\nSystemsManager\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ssm:PutParameter\",\n        \"ssm:GetParametersByPath\",\n        \"ssm:GetParameters\",\n        \"ssm:GetParameter\",\n        \"ssm:AddTagsToResource\"\n      ],\n      \"Resource\": \"arn:aws:ssm:&lt;aws_region&gt;:&lt;aws_account_number&gt;:parameter/*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"ssm:DescribeParameters\",\n      \"Resource\": \"*\"\n    }\n  ]\n}\n\n\nTAG (Tag editor)\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"tag:GetResources\",\n        \"tag:GetTagValues\",\n        \"tag:GetTagKeys\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}"
  },
  {
    "objectID": "posts/2024-02-25-how-to-set-up-development-and-production-environments-using-aws-copilot-example-using-a-plumber-api/index.html#gh-actions",
    "href": "posts/2024-02-25-how-to-set-up-development-and-production-environments-using-aws-copilot-example-using-a-plumber-api/index.html#gh-actions",
    "title": "How to set up development and production environments using AWS Copilot: Example using a plumber API.",
    "section": "GH Actions",
    "text": "GH Actions\nUse this tutorial to create a IAM role for GitHub Actions: https://aws.amazon.com/blogs/security/use-iam-roles-to-connect-github-actions-to-actions-in-aws/\n\nPolicy for GH Actions role\nThe policies for GH Actions has reduced permissions. It is added to the role created above.\n\nTrust Relationship\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Federated\": \"arn:aws:iam::&lt;aws_account_number&gt;:oidc-provider/token.actions.githubusercontent.com\"\n            },\n            \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"token.actions.githubusercontent.com:aud\": \"sts.amazonaws.com\"\n                },\n                \"StringLike\": {\n                    \"token.actions.githubusercontent.com:sub\": \"repo:myorg/myrepo:ref:refs/*\"\n                }\n            }\n        }\n    ]\n}\n\n\nSTS\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"sts:AssumeRole\",\n        \"sts:GetCallerIdentity\",\n        \"sts:AssumeRoleWithWebIdentity\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n\n\n\nDeploy\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Resource\": \"arn:aws:ssm:&lt;aws_region&gt;:&lt;aws_account_number&gt;:parameter/copilot/*\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ssm:GetParametersByPath\",\n        \"ssm:GetParameter\"\n      ]\n    },\n    {\n      \"Resource\": \"arn:aws:cloudformation:&lt;aws_region&gt;:&lt;aws_account_number&gt;:stackset/myapi-infrastructure:*\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"cloudformation:ListStackInstances\"\n      ]\n    },\n    {\n      \"Resource\": \"arn:aws:cloudformation:&lt;aws_region&gt;:&lt;aws_account_number&gt;:stack/*\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"cloudformation:DescribeStacks\"\n      ]\n    },\n    {\n      \"Resource\": \"*\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ecr:GetAuthorizationToken\"\n      ]\n    },\n    {\n      \"Resource\": [\n        \"arn:aws:ecr:&lt;aws_region&gt;:&lt;aws_account_number&gt;:repository/myapi/*\",\n        \"arn:aws:ecr:&lt;aws_region&gt;:&lt;aws_account_number&gt;:repository/myapi_base\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ecr:InitiateLayerUpload\",\n        \"ecr:UploadLayerPart\",\n        \"ecr:CompleteLayerUpload\",\n        \"ecr:PutImage\",\n        \"ecr:BatchCheckLayerAvailability\",\n        \"ecr:BatchGetImage\",\n        \"ecr:GetDownloadUrlForLayer\"\n      ]\n    }\n  ]\n}\n\n\nDeploy to dev/stage GH Action\nDeploy to dev/stage is triggered by push or pull request to the corresponding branches.\n# This is a basic workflow to help you get started with Actions\nname: Connect to an AWS role from a GitHub repository\n\n# Controls when the action will run. Invokes the workflow on push events but only for the main branch\non:\n  push:\n    branches: [dev]\n  pull_request:\n    branches: [dev]\n\nenv:\n  AWS_REGION: \"&lt;aws_region&gt;\" #Change to reflect your Region\n\n# Permission can be added at job level or workflow level\npermissions:\n  id-token: write # This is required for requesting the JWT\n  contents: read # This is required for actions/checkout\njobs:\n  DeployService:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Git clone the repository\n        uses: actions/checkout@v4\n      - name: configure aws credentials\n        uses: aws-actions/configure-aws-credentials@v1.7.0\n        with:\n          role-to-assume: arn:aws:iam::&lt;aws_account_number&gt;:role/GitHubAction-AssumeRoleWithAction #change to reflect your IAM role’s ARN\n          role-session-name: GitHub_to_AWS_via_FederatedOIDC\n          aws-region: ${{ env.AWS_REGION }}\n      # Hello from AWS: WhoAmI\n      # - name: Sts GetCallerIdentity\n      #   run: |\n      #     aws sts get-caller-identity\n      - name: Install copilot\n        run: |\n          mkdir -p $GITHUB_WORKSPACE/bin\n          # download copilot\n          curl -Lo copilot-linux https://github.com/aws/copilot-cli/releases/latest/download/copilot-linux && \\\n          # make copilot bin executable\n          chmod +x copilot-linux && \\\n          # move to path\n          mv copilot-linux $GITHUB_WORKSPACE/bin/copilot && \\\n          # add to PATH\n          echo \"$GITHUB_WORKSPACE/bin\" &gt;&gt; $GITHUB_PATH\n          # - run: copilot help\n      - name: deploy service\n        run: copilot svc deploy --env dev\n\n\n\nDeploy to prod GH Action\nDeploy to PROD is triggered by manually creating a release in GitHub.\n# This is a basic workflow to help you get started with Actions\nname: Connect to an AWS role from a GitHub repository\n\n# Controls when the action will run. Invokes the workflow on push events but only for the main branch\non:\n  release:\n    types: [published]\n\nenv:\n  AWS_REGION: \"&lt;aws_region&gt;\" #Change to reflect your Region\n\n# Permission can be added at job level or workflow level\npermissions:\n  id-token: write # This is required for requesting the JWT\n  contents: read # This is required for actions/checkout\njobs:\n  DeployService:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Git clone the repository\n        uses: actions/checkout@v4\n      - name: configure aws credentials\n        uses: aws-actions/configure-aws-credentials@v1.7.0\n        with:\n          role-to-assume: arn:aws:iam::&lt;aws_account_number&gt;:role/GitHubAction-AssumeRoleWithAction #change to reflect your IAM role’s ARN\n          role-session-name: GitHub_to_AWS_via_FederatedOIDC\n          aws-region: ${{ env.AWS_REGION }}\n      # Hello from AWS: WhoAmI\n      # - name: Sts GetCallerIdentity\n      #   run: |\n      #     aws sts get-caller-identity\n      - name: Install copilot\n        run: |\n          mkdir -p $GITHUB_WORKSPACE/bin\n          # download copilot\n          curl -Lo copilot-linux https://github.com/aws/copilot-cli/releases/latest/download/copilot-linux && \\\n          # make copilot bin executable\n          chmod +x copilot-linux && \\\n          # move to path\n          mv copilot-linux $GITHUB_WORKSPACE/bin/copilot && \\\n          # add to PATH\n          echo \"$GITHUB_WORKSPACE/bin\" &gt;&gt; $GITHUB_PATH\n          # - run: copilot help\n      - name: deploy service\n        run: copilot svc deploy --env prod"
  },
  {
    "objectID": "posts/2024-01-15-a-simple-workflow-for-async-shiny-with-mirai/index.html",
    "href": "posts/2024-01-15-a-simple-workflow-for-async-shiny-with-mirai/index.html",
    "title": "A simple workflow for async {shiny} with {mirai}",
    "section": "",
    "text": "In my previous post, I developed a shiny module to encapsulate the logic of sending and monitoring background async tasks. The main advantage of this approach was to simplify making repeated async calls in larger applications. In the first version of this module, the async process was created with callr:r_bg, an approach that my self and others have used before.\nHowever, there is one, potentially significant, drawback of using callr in such a way. Take this hypotetical scenario as an example. You have a shiny app with five async tasks triggered in response to a user changing a dataset. You test it locally, and everything works great. Then you deploy and share with the world. Ten of your followers click on the link more-or-less at the same time and visit the application, each choosing one of three datasets available in your data science app. The app’s server, featuring async functions gets to work, and initializes 5 (tasks) * 10 (users) = 50 callr::r_bg calls, each running in a separate child R process. Some of these copy nothing the child enviroment, some only a few small objects, but others a large data object needed for the async function to transform or run a model. It should be no surprise if the app is no longer that fast. The hosting server, even with a fast, multi-thread processor, still hast to contend with many R processes and the shiny session is also getting a bit bogged down, as it has potentially dozens of observers monitoring background processes. Clearly, we need to rethink our approach.\nWouldn’t it be great if we had a way to limit the total number of concurrent child R processes that our shiny session would spawn, and have a queue system that would start another background job as soon as one completes? Enter mirai. mirai lets us initialize a set number of R daemons (persistent background processes) that are ready to receive mirai requests and ensures FIFO (first in, first out) scheduling. Using mirai, we can handle a large number of async background jobs elegantly without overburdening the system. If the number of jobs requested by the shiny app exceeds the number of available daemons, mirai would hold the jobs until one of the daemons (threads) frees up and submit on a first-come, first-serve basis. Just great!"
  },
  {
    "objectID": "posts/2024-01-15-a-simple-workflow-for-async-shiny-with-mirai/index.html#so-how-does-it-work",
    "href": "posts/2024-01-15-a-simple-workflow-for-async-shiny-with-mirai/index.html#so-how-does-it-work",
    "title": "A simple workflow for async {shiny} with {mirai}",
    "section": "So how does it work?",
    "text": "So how does it work?\nFor example setups for shiny, check out the documentation, where you can read about mirai-only solutions, as well as approaches combining mirai with promises.\nFor my application, I’ll adapt the callr approach I described in my previous post to work with mirai. In fact, there very little to change to make the callr example work with mirai:\n\nChange the async version of our function to use mirai\n\n\nhead_six &lt;- function(x, sleep) {\n  Sys.sleep(sleep)\n  head(x)\n}\n\nhead_six_async_mirai &lt;- function(x, sleep) {\n  args &lt;- list(head_six = head_six, x = x, sleep = sleep)\n  bg_process &lt;- mirai::mirai(.expr = head_six(x, sleep), .args = args)\n  return(bg_process)\n}\n\n\nChange the polling logic in the module’s server to use mirai::unresolved, rather than the is_alive method of the callr process object.\n\n\nmod_async_srv_mirai &lt;- function(id, fun_async, fun_args, wait_for_event = FALSE, verbose = FALSE) {\n  moduleServer( id, function(input, output, session){\n    res_rct &lt;- shiny::reactiveVal(NULL)\n    poll_rct &lt;- shiny::reactiveVal(TRUE)\n\n    if (isTRUE(wait_for_event)) {\n      poll_rct(FALSE)\n    }\n\n    bg_job &lt;- reactive({\n      req(isTRUE(poll_rct()))\n      do.call(fun_async, fun_args)\n    }) |&gt; bindEvent(poll_rct())\n\n    observe({\n      req(isTRUE(poll_rct()))\n      invalidateLater(250)\n      if (verbose) {\n        message(sprintf(\"checking: %s\", id))\n      }\n\n      alive &lt;- mirai::unresolved(bg_job())\n      if (isFALSE(alive)) {\n        res_rct(bg_job()$data)\n        if (verbose) {\n          message(sprintf(\"done: %s\", id))\n        }\n        poll_rct(FALSE)\n      }\n    })\n\n    return(list(\n      start_job = function() poll_rct(TRUE),\n      get_result = reactive(res_rct())\n    ))\n\n  })\n}\n\n\nIn the app’s server, or better yet global.R or equivalents, we need to initialize the daemons:\n\n\n  mirai::daemons(2L)\n  onStop(function() mirai::daemons(0L))\n\nIn this setup, our shiny can run up to two parallel async jobs handled by the mirai queue. These daemons are shared across all users of our application, irrespective of the shiny session. This is because mirai’s daemons apply to the entire R session, not individual shiny sessions."
  },
  {
    "objectID": "posts/2024-01-15-a-simple-workflow-for-async-shiny-with-mirai/index.html#gist",
    "href": "posts/2024-01-15-a-simple-workflow-for-async-shiny-with-mirai/index.html#gist",
    "title": "A simple workflow for async {shiny} with {mirai}",
    "section": "Gist",
    "text": "Gist\nFor a running example of mirai async with the module, visit this gist:"
  },
  {
    "objectID": "posts/2024-01-15-a-simple-workflow-for-async-shiny-with-mirai/index.html#summary",
    "href": "posts/2024-01-15-a-simple-workflow-for-async-shiny-with-mirai/index.html#summary",
    "title": "A simple workflow for async {shiny} with {mirai}",
    "section": "Summary",
    "text": "Summary\nIn this post I went over an approach to organize mirai background async jobs using a shiny module, in order to make the async code faster to write, less error prone and overall cleaner."
  },
  {
    "objectID": "posts/2023-02-04-deploy-an-r-script-as-an-aws-lambda-function-without-leaving-the-r-console/index.html",
    "href": "posts/2023-02-04-deploy-an-r-script-as-an-aws-lambda-function-without-leaving-the-r-console/index.html",
    "title": "Deploy an R script as an AWS Lambda function without leaving the R console",
    "section": "",
    "text": "AWS Lambda is a serverless compute service that lets us deploy any code as a cloud function without worrying about setting up a compute server (for example like discussed here and here). The code can be deployed as a zip archive or a container image. In the R case, only the container option is available and enabled by the {lambdr} package (see also this workflow). The deployment using {lambdr}procedure involves:\n\nwriting an R script to serve as the runtime function\n\ncreating a Lambda Dockerfile and docker image locally\n\npushing the docker image to AWS Elastic Container Registry (ECR)\n\ncreating a Lambda function using the ECR image either with the AWS command-line interface (cli) or in the web console\n\nGoing through this procedure is a rewarding experience, as it teaches many things related to docker containers, AWS setup, using the AWS cli, etc. But, once you have gone through the this procedure a few times, it becomes a bit cumbersome and time-consuming to navigate between an R console, the shell, and aws cli or the AWS web console in a browser to write and update the code, re-create the docker image, push, and then re-create the Lambda function. It would be nice to have a “one-call” solution, where we call a single function, point it to a file that contains the code we wish to deploy, and sit back while our R session goes through the steps.\nIn this post, I will introduce an R package, called {r2lambda}, that aims to make it easier to deploy R code as AWS Lambda function by automating the above procedure."
  },
  {
    "objectID": "posts/2023-02-04-deploy-an-r-script-as-an-aws-lambda-function-without-leaving-the-r-console/index.html#system-dependencies",
    "href": "posts/2023-02-04-deploy-an-r-script-as-an-aws-lambda-function-without-leaving-the-r-console/index.html#system-dependencies",
    "title": "Deploy an R script as an AWS Lambda function without leaving the R console",
    "section": "System dependencies",
    "text": "System dependencies\nThe only system dependency is docker, because R lambdas are docker images, so we are not going anywhere unless docker is installed."
  },
  {
    "objectID": "posts/2023-02-04-deploy-an-r-script-as-an-aws-lambda-function-without-leaving-the-r-console/index.html#r-dependencies",
    "href": "posts/2023-02-04-deploy-an-r-script-as-an-aws-lambda-function-without-leaving-the-r-console/index.html#r-dependencies",
    "title": "Deploy an R script as an AWS Lambda function without leaving the R console",
    "section": "R dependencies",
    "text": "R dependencies\nThe core R dependencies are {lambdr}, {stevedore}, and {paws}.\n{lambdr} provides the R runtime for AWS Lambda. In practice, the most important points about using {lambdr} are 1) to setup the Dockerfile:\n\ninstallation of system dependencies\n\ninstallation of R dependencies of the runtime function\n\nsetting the runtime function to be run by the container via CMD,\n\nand 2) to setup the R script by adding lambdr::start_lambda() at the bottom.\n{stevedore} is a docker client for R. It provides an interface to the docker API. In the context of deploiyng Lambda functions, it is used to list and tag local images, and to login and push images to the AWS ECR repository. Using {stevedore} simplifies how {r2lambda} works in two ways. First, we don’t need to use system calls to run {docker} commands, and second, we don’t depend on the aws cli.\n{paws} is the R software development kit for AWS. {r2lambda} uses {paws} to connect to the AWS services using your credentials, to create execution roles for the Lambda, to create the Lambda it self, and to invoke the Lambda function.\nThe remaining R dependencies provide features for input validation and testing ({checkmate}), logging ({logger}), and text interpolation ({glue})."
  },
  {
    "objectID": "posts/2023-02-04-deploy-an-r-script-as-an-aws-lambda-function-without-leaving-the-r-console/index.html#environmental-variables-for-aws-configuration",
    "href": "posts/2023-02-04-deploy-an-r-script-as-an-aws-lambda-function-without-leaving-the-r-console/index.html#environmental-variables-for-aws-configuration",
    "title": "Deploy an R script as an AWS Lambda function without leaving the R console",
    "section": "Environmental variables for AWS configuration",
    "text": "Environmental variables for AWS configuration\nOur AWS credentials are needed so that functions that use the paws SDK can authenticate with AWS. This is a simple .Renviron:\nACCESS_KEY_ID = \"YOUR AWS ACCESS KEY ID\"\nSECRET_ACCESS_KEY = \"YOUR AWS SECRET ACCESS KEY\"\nPROFILE = \"YOUR AWS PROFILE\"\nREGION = \"YOUR AWS REGION\""
  },
  {
    "objectID": "posts/2023-02-04-deploy-an-r-script-as-an-aws-lambda-function-without-leaving-the-r-console/index.html#installation",
    "href": "posts/2023-02-04-deploy-an-r-script-as-an-aws-lambda-function-without-leaving-the-r-console/index.html#installation",
    "title": "Deploy an R script as an AWS Lambda function without leaving the R console",
    "section": "Installation",
    "text": "Installation\nOnce the prerequisites are ready, we can install {r2lambda} from GitHub using {remotes}:\n# install_packages(\"remotes\")\nremotes::install_github(\"discindo\", \"r2lambda\")"
  },
  {
    "objectID": "posts/2023-02-04-deploy-an-r-script-as-an-aws-lambda-function-without-leaving-the-r-console/index.html#demo-run-with-logs",
    "href": "posts/2023-02-04-deploy-an-r-script-as-an-aws-lambda-function-without-leaving-the-r-console/index.html#demo-run-with-logs",
    "title": "Deploy an R script as an AWS Lambda function without leaving the R console",
    "section": "Demo run with logs",
    "text": "Demo run with logs\nA full deployment and invocation run with a demo runtime script should look like the code and output below.\n&gt;   runtime_function &lt;- \"parity\"\n&gt;   runtime_path &lt;- system.file(\"parity.R\", package = \"r2lambda\")\n&gt;   dependencies &lt;- NULL\n&gt; \n&gt;   deploy_lambda(\n+     tag = \"parity-test1\",\n+     runtime_function = runtime_function,\n+     runtime_path = runtime_path,\n+     dependencies = dependencies\n+     )\nINFO [2023-01-29 20:32:41] [deploy_lambda] Checking system dependencies (`aws cli`, `docker`).\n/usr/bin/docker\nINFO [2023-01-29 20:32:41] [deploy_lambda] Creating temporary working directory.\nINFO [2023-01-29 20:32:41] [deploy_lambda] Creating Dockerfile.\nWARN [2023-01-29 20:32:41] [deploy_lambda] Created Dockerfile and lambda runtime script in temporary folder.\nINFO [2023-01-29 20:32:41] [deploy_lambda] Building Docker image.\nSending build context to Docker daemon  3.584kB\nStep 1/13 : FROM public.ecr.aws/lambda/provided\n ---&gt; ccae8d728af2\nStep 2/13 : ENV R_VERSION=4.0.3\n ---&gt; Using cache\n ---&gt; bf3dd3c804f3\nStep 3/13 : RUN yum -y install wget git tar\n ---&gt; Using cache\n ---&gt; 8b82b80771cf\nStep 4/13 : RUN yum -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm   && wget https://cdn.rstudio.com/r/centos-7/pkgs/R-${R_VERSION}-1-1.x86_64.rpm   && yum -y install R-${R_VERSION}-1-1.x86_64.rpm   && rm R-${R_VERSION}-1-1.x86_64.rpm\n ---&gt; Using cache\n ---&gt; c98bc560eff4\nStep 5/13 : ENV PATH=\"${PATH}:/opt/R/${R_VERSION}/bin/\"\n ---&gt; Using cache\n ---&gt; 4565b8100c39\nStep 6/13 : RUN yum -y install openssl-devel\n ---&gt; Using cache\n ---&gt; d8e46fe52a6d\nStep 7/13 : RUN Rscript -e \"install.packages(c('httr', 'jsonlite', 'logger', 'remotes'), repos = 'https://packagemanager.rstudio.com/all/__linux__/centos7/latest')\"\n ---&gt; Using cache\n ---&gt; 46ca4b6e95a0\nStep 8/13 : RUN Rscript -e \"remotes::install_github('mdneuzerling/lambdr')\"\n ---&gt; Using cache\n ---&gt; 67283a940985\nStep 9/13 : RUN mkdir /lambda\n ---&gt; Using cache\n ---&gt; d6762390f9a9\nStep 10/13 : COPY runtime.R /lambda\n ---&gt; Using cache\n ---&gt; 94af1e345ecc\nStep 11/13 : RUN chmod 755 -R /lambda\n ---&gt; Using cache\n ---&gt; cd15870ad843\nStep 12/13 : RUN printf '#!/bin/sh\\ncd /lambda\\nRscript runtime.R' &gt; /var/runtime/bootstrap   && chmod +x /var/runtime/bootstrap\n ---&gt; Using cache\n ---&gt; 66d74d4de62e\nStep 13/13 : CMD [\"parity\"]\n ---&gt; Using cache\n ---&gt; e47d4fea17a1\nSuccessfully built e47d4fea17a1\nSuccessfully tagged parity-test1:latest\nWARN [2023-01-29 20:32:41] [deploy_lambda] Docker image built. This can take up substantial amount of disk space.\nWARN [2023-01-29 20:32:41] [deploy_lambda] Use `docker image ls` in your shell to see the image size.\nWARN [2023-01-29 20:32:41] [deploy_lambda] Use `docker rmi &lt;image&gt;` in your shell to remove an image.\nINFO [2023-01-29 20:32:41] [deploy_lambda] Pushing Docker image to AWS ECR.\n\n... [truncated]\n\nLogin Succeeded\nThe push refers to repository [*.dkr.ecr.us-east-1.amazonaws.com/parity-test1]\n\n... [truncated]\n\nlatest: digest: sha256:9f38150cf89bf6a3f7d95c853105afe82616f85f3afbb65d7f71d2f1400dedeb size: 3045\nWARN [2023-01-29 20:45:25] [deploy_lambda] Docker image pushed to ECR. This can take up substantial resources and incur cost.\nWARN [2023-01-29 20:45:25] [deploy_lambda] Use `paws::ecr()`, the AWS CLI, or the AWS console to manage your images.\nINFO [2023-01-29 20:45:25] [deploy_lambda] Creating Lambda role and basic policy.\nWARN [2023-01-29 20:45:26] [deploy_lambda] Created AWS role with basic lambda execution permissions.\nWARN [2023-01-29 20:45:26] [deploy_lambda] Use `paws::iam()`, the AWS CLI, or the AWS console to manage your roles, and permissions.\nINFO [2023-01-29 20:45:36] [deploy_lambda] Creating Lambda function from image.\nWARN [2023-01-29 20:45:37] [deploy_lambda] Lambda function created. This can take up substantial resources and incur cost.\nWARN [2023-01-29 20:45:37] [deploy_lambda] Use `paws::lambda()`, the AWS CLI, or the AWS console to manage your functions.\nWARN [2023-01-29 20:45:37] [deploy_lambda] Lambda function created successfully.\nWARN [2023-01-29 20:45:37] [deploy_lambda] Pushed docker image to ECR with URI `*.dkr.ecr.us-east-1.amazonaws.com/parity-test1`\nWARN [2023-01-29 20:45:37] [deploy_lambda] Created Lambda execution role with ARN `arn:aws:iam::*:role/parity-test1--261b7f62-a048-11ed-bd89-10c37b6dce99`\nWARN [2023-01-29 20:45:37] [deploy_lambda] Created Lambda function `parity-test1` with ARN `arn:aws:lambda:us-east-1:*:function:parity-test1`\nSUCCESS [2023-01-29 20:45:37] [deploy_lambda] Done.\n&gt; \n&gt;  invoke_lambda(\n+    function_name = \"parity-test1\",\n+    payload = list(number = 3),\n+    invocation_type = \"RequestResponse\"\n+   )\nINFO [2023-01-29 20:45:37] [invoke_lambda] Validating inputs.\nINFO [2023-01-29 20:45:37] [invoke_lambda] Invoking function.\nError: ResourceConflictException (HTTP 409). The operation cannot be performed at this time. The function is currently in the following state: Pending\n\n&gt;  invoke_lambda(\n+    function_name = \"parity-test1\",\n+    payload = list(number = 3),\n+    invocation_type = \"RequestResponse\"\n+   )\nINFO [2023-01-29 21:32:53] [invoke_lambda] Validating inputs.\nINFO [2023-01-29 21:32:53] [invoke_lambda] Invoking function.\n\nLambda response payload: \n{\"parity\":\"odd\"}\nSUCCESS [2023-01-29 21:33:02] [invoke_lambda] Done.\nThe code is purposely verbose to let the user know what actions are being taken and what resources are being set up on AWS. Of course, setting up and running services on AWS incurs costs, so it is always a good idea to review your AWS console and disable or remove services that are no longer needed. Actions like these can also be done with the paws SDK, and I hope that a future iteration of {r2lamdba} might make some AWS clean up possible from the R console. For now, deploy_lambda will specify the URI or ARN of each service it creates both as a console log, and in the returned list. So the user can easily find the created services and disable/remove/update as needed. As of now, {r2lambda} interacts only with IAM, ECR, and Lambda, so be sure at least to log into AWS, and review the actions taken."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About us\nWelcome to discindo.org! This is a space where we document our learning and discoveries as we navigate through the data science + data engineering universe. We mostly focus on R and Shiny and how to build data solutions using these in the cloud.\nOur projects page has packages and tools we have developed as free software.\nOur website has description of previous projects and references from our clients.\nWe would love to hear from you! Please get in touch through the contact page, LinkedIn, or Twitter.\nIf you find this blog useful, consider supporting us on Patreon or Ko-fi.\nNovica\nTeo"
  },
  {
    "objectID": "posts/2024-10-17-r2lambda-renv/index.html",
    "href": "posts/2024-10-17-r2lambda-renv/index.html",
    "title": "r2lambda update to support multi-file projects and renv projects",
    "section": "",
    "text": "It has been a while since I’ve had the chance to work on my {r2lambda} project. In particular, there were a couple of good points made by a user on GitHub about functionality that is missing from the package. The option to deploy multiple files, e.g., one runtime function that depends on helpers in the same project organized in different files. And another, to enable {renv} management of the R environment within the AWS Lambda docker image. Both excellent points that I wished I addressed earlier. But better late than never.\nBoth of these features required minor adjustments to the codebase. Copying additional supports scripts and restoring the {renv} environment should both happen when the AWS Lambda docker image is built, so the logic to create the Dockerfile needed to be updated. Accordinly, the r2lambda::build_lambda function now has two additional arguments:\n#' @param support_path path to the support files (if any). Either NULL \n#' (the default) if all needed code is in the same `runtime_path` script, or a \n#' character vector of paths to additional files needed by the runtime script.\n#' @param renvlock_path path to the renv.lock file (if any). Default is NULL.\n#' \n#' @details Use either `renvlock_path` or `dependencies` to install required\n#' packages, not both. By default, both are `NULL`, so the Docker image will\n#' have no additional packages installed.\nTo include any support scripts, provide a character vector script paths to the support_path argument when building the Lambda docker image locally with build_lamdba.\n[Note that, multi-file project was supported previously as well, although perhaps not explicitly. An approach that I like is to create an ‘R’ package that exports the runtime function needed for the Lambda. Then one just needs to make that custom R package a dependency of the project and either install in the AWS Lambda docker image it through dependencies or renvlock_path.]\nTo use an existing renv.lock for installation of dependencies, provide its path to the renvlock_path argument to build_lambda. This instructs the code to copy the renv.lock file to the image and run renv::restore() which will reconstruct the R environment inside the docker image. I really like this feature, as it minimizes the size of the Dockefile and removes some potential headaches with R package dependencies from different repositories (CRAN, BioConductor, GitHub, etc)."
  },
  {
    "objectID": "posts/2024-10-17-r2lambda-renv/index.html#deploy-a-project-with-multiple-r-scripts-and-renv-managed-environment-to-aws-lambda",
    "href": "posts/2024-10-17-r2lambda-renv/index.html#deploy-a-project-with-multiple-r-scripts-and-renv-managed-environment-to-aws-lambda",
    "title": "r2lambda update to support multi-file projects and renv projects",
    "section": "",
    "text": "It has been a while since I’ve had the chance to work on my {r2lambda} project. In particular, there were a couple of good points made by a user on GitHub about functionality that is missing from the package. The option to deploy multiple files, e.g., one runtime function that depends on helpers in the same project organized in different files. And another, to enable {renv} management of the R environment within the AWS Lambda docker image. Both excellent points that I wished I addressed earlier. But better late than never.\nBoth of these features required minor adjustments to the codebase. Copying additional supports scripts and restoring the {renv} environment should both happen when the AWS Lambda docker image is built, so the logic to create the Dockerfile needed to be updated. Accordinly, the r2lambda::build_lambda function now has two additional arguments:\n#' @param support_path path to the support files (if any). Either NULL \n#' (the default) if all needed code is in the same `runtime_path` script, or a \n#' character vector of paths to additional files needed by the runtime script.\n#' @param renvlock_path path to the renv.lock file (if any). Default is NULL.\n#' \n#' @details Use either `renvlock_path` or `dependencies` to install required\n#' packages, not both. By default, both are `NULL`, so the Docker image will\n#' have no additional packages installed.\nTo include any support scripts, provide a character vector script paths to the support_path argument when building the Lambda docker image locally with build_lamdba.\n[Note that, multi-file project was supported previously as well, although perhaps not explicitly. An approach that I like is to create an ‘R’ package that exports the runtime function needed for the Lambda. Then one just needs to make that custom R package a dependency of the project and either install in the AWS Lambda docker image it through dependencies or renvlock_path.]\nTo use an existing renv.lock for installation of dependencies, provide its path to the renvlock_path argument to build_lambda. This instructs the code to copy the renv.lock file to the image and run renv::restore() which will reconstruct the R environment inside the docker image. I really like this feature, as it minimizes the size of the Dockefile and removes some potential headaches with R package dependencies from different repositories (CRAN, BioConductor, GitHub, etc)."
  },
  {
    "objectID": "posts/2024-10-17-r2lambda-renv/index.html#demo-code",
    "href": "posts/2024-10-17-r2lambda-renv/index.html#demo-code",
    "title": "r2lambda update to support multi-file projects and renv projects",
    "section": "Demo code",
    "text": "Demo code\nAssuming we have a folder with the following structure:\n~/Desktop$ ls -1 iris-lambda/\nrenv/\nrenv.lock\nruntime.r\nsupport.r\ntest-code.r\nWhere, support.r defines some function that runtime.r uses for the Lambda:\nget_iris_summary_by_species &lt;- function(species) {\n    iris |&gt;\n    dplyr::filter(Species == species) |&gt;\n    dplyr::summarise(\n      mean = mean(Sepal.Length),\n      sd = sd(Sepal.Length)\n    )\n}\nThen runtime.r, sources the support script, and calls the function defined there:\nsource(\"support.r\")\n\niris_summary &lt;- function(species) {\n  get_iris_summary_by_species(species)\n}\n\nlambdr::start_lambda()\nThen the following should work, passing the support script and renv.lock to r2lambda::build_lambda:\ndir(\"~/Desktop/iris-lambda\")\nruntime_function &lt;- \"iris_summary\"\nruntime_path &lt;- \"~/Desktop/iris-lambda/runtime.r\"\nsupport_path &lt;- \"~/Desktop/iris-lambda/support.r\"\nrenvlock_path &lt;- \"~/Desktop/iris-lambda/renv.lock\"\ndependencies &lt;- NULL\n\n# Might take a while, its building a docker image\nbuild_lambda(\n  tag = \"my_iris_lambda\",\n  runtime_function = runtime_function,\n  runtime_path = runtime_path,\n  support_path = support_path,\n  renvlock_path = renvlock_path,\n  dependencies = dependencies\n)\n\n# test\npayload &lt;- list(species = \"setosa\")\ntag &lt;- \"my_iris_lambda\"\ntest_lambda(tag = tag, payload)\n\n\n# deploy\n\n# Might take a while, its pushing it to a remote repository\ndeploy_lambda(\n  tag = \"my_iris_lambda\",\n  Timeout = 30\n)\n\ninvoke_lambda(\n  function_name = \"my_iris_lambda\",\n  invocation_type = \"RequestResponse\",\n  payload = list(species = \"versicolor\"),\n  include_logs = FALSE\n)\n\ninvoke_lambda(\n  function_name = \"my_iris_lambda\",\n  invocation_type = \"RequestResponse\",\n  payload = list(species = \"setosa\"),\n  include_logs = FALSE\n)"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "100 days of Python and R\n\n\n\n\n\n\n\n\n\n\n\nDec 29, 2023\n\n\nnovica\n\n\n\n\n\n\n\n\n\n\n\n\nA guide for reproducible data analysis in Macedonian\n\n\n\n\n\n\n\n\n\n\n\nDec 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nA simple workflow for async {shiny} with {callr}\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2024\n\n\nteo\n\n\n\n\n\n\n\n\n\n\n\n\nA simple workflow for async {shiny} with {mirai}\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2024\n\n\nteo\n\n\n\n\n\n\n\n\n\n\n\n\nA {shiny} app to wrap BlasterJS and visualize NCBI blast results locally\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nAn R AWS Lambda function to download Tidytuesday datasets\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\nteo\n\n\n\n\n\n\n\n\n\n\n\n\nAsynchronous background execution in Shiny using callr\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nBasic visual manupulation of phylogenies in R\n\n\n\n\n\n\n\n\n\n\n\nDec 18, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a multi-session {shiny} application with {brochure}\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nData wrangling tricks from the R4DS slack\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\nnovica\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy an R script as an AWS Lambda function without leaving the R console\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2023\n\n\nTeo\n\n\n\n\n\n\n\n\n\n\n\n\nDeploying Plumber API to AWS Elastic Container Service\n\n\n\n\n\n\n\n\n\n\n\nDec 28, 2023\n\n\nnovica\n\n\n\n\n\n\n\n\n\n\n\n\nFew notes on getting R package data from the local library\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2024\n\n\nnovica\n\n\n\n\n\n\n\n\n\n\n\n\nFew things I learned while writing an R package\n\n\n\n\n\n\n\n\n\n\n\nJul 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Google Analytics data in R the long way\n\n\n\n\n\n\n\n\n\n\n\nDec 29, 2022\n\n\nnovica\n\n\n\n\n\n\n\n\n\n\n\n\nHow to deploy Shiny application to Digital Ocean using GitHub Actions\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2023\n\n\nteo\n\n\n\n\n\n\n\n\n\n\n\n\nHow to invoke an AWS Lambda function with R and paws\n\n\n\n\n\n\n\n\n\n\n\nJul 24, 2022\n\n\nTeo\n\n\n\n\n\n\n\n\n\n\n\n\nHow to set up an R-based AWS Lambda to write to AWS S3 on a schedule\n\n\nUse AWS Lambda to save the Tidytuesday dataset to AWS S3 every Wednesday\n\n\n\n\n\n\n\n\nMar 9, 2023\n\n\nteo\n\n\n\n\n\n\n\n\n\n\n\n\nHow to set up development and production environments using AWS Copilot: Example using a plumber API.\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2024\n\n\nTeo\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use Bootstrap 5 popovers in Shiny applications\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2023\n\n\nteo\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use buttons in a Reactable widget for navigation in a Shiny application\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\nteo\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use custom icons in Rmd reports and Shiny applications\n\n\n\n\n\n\n\n\n\n\n\nMar 19, 2023\n\n\nteo\n\n\n\n\n\n\n\n\n\n\n\n\nJoining the flock from R: working with data on MotherDuck\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2024\n\n\nnovica\n\n\n\n\n\n\n\n\n\n\n\n\nNoting the differences in deploying R vs Python apps on Posit Connect\n\n\n\n\n\n\n\n\n\n\n\nSep 29, 2024\n\n\nnovica\n\n\n\n\n\n\n\n\n\n\n\n\nOpenBudget: A Shiny app for transparent finances in local government in North Macedonia\n\n\n\n\n\n\n\n\n\n\n\nMay 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nPackaging a Python Shiny app\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2022\n\n\nnovica\n\n\n\n\n\n\n\n\n\n\n\n\nPython dashboards after Shiny for Python\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2022\n\n\nnovica\n\n\n\n\n\n\n\n\n\n\n\n\nRstudio in the cloud for those of us with old laptops\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2023\n\n\nnovica\n\n\n\n\n\n\n\n\n\n\n\n\nRstudio in the cloud for those of us with old laptops part 2: automating with terraform\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2023\n\n\nnovica\n\n\n\n\n\n\n\n\n\n\n\n\nSet an R-based AWS Lambda function to run on a schedule\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\nteo\n\n\n\n\n\n\n\n\n\n\n\n\nShiny ducks: connecting to MotherDuck from Shiny\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2024\n\n\nnovica\n\n\n\n\n\n\n\n\n\n\n\n\nSix months of #rstats workshops in Skopje, and more\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nTest an R-based AWS lambda function locally before deploying to the cloud\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2023\n\n\nteo\n\n\n\n\n\n\n\n\n\n\n\n\nUsing GitHub actions with R: Some notes from our #TidyTuesday setup\n\n\n\n\n\n\n\n\n\n\n\nDec 29, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nUsing R and {paws} to populate DynamoDB tables\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nUsing R and {paws} to populate DynamoDB tables #2\n\n\n\n\n\n\n\n\n\n\n\nMay 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Wikidata to draw networks of Politically Exposed Persons #1\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2022\n\n\nnovica\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Wikidata to draw networks of Politically Exposed Persons #2\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\nnovica\n\n\n\n\n\n\n\n\n\n\n\n\nUsing uv to manage the environment for a Python Shiny app and setting up a GitHub action to publish it to Posit Connect\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2024\n\n\nnovica\n\n\n\n\n\n\n\n\n\n\n\n\nUsing {shiny.i18n} with {golem} for server-side translation\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nUsing {stevedore} to manage docker-based variant-calling pipelines from R\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2023\n\n\nteo\n\n\n\n\n\n\n\n\n\n\n\n\nWindsoraiR package for accessing Windsor.ai API from R\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nr2lambda update to support multi-file projects and renv projects\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2024\n\n\nTeo\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/r2lambda/2023-02-19-test-an-r-based-aws-lambda-function-locally/index.html",
    "href": "posts/r2lambda/2023-02-19-test-an-r-based-aws-lambda-function-locally/index.html",
    "title": "Test an R-based AWS lambda function locally before deploying to the cloud",
    "section": "",
    "text": "Creating AWS Lambda functions from R code can be a powerful way to make our local R code available in the cloud as an on-demand serverless service. In recent weeks I’ve been working on my package r2lambda to build and deploy AWS Lambda functions from the R console. In an introductory article about the package a few weeks ago, I covered the basics and showcased the main usage:\n\n# install_packages(\"remotes\")\nremotes::install_github(\"discindo/r2lambda\")\n\nr2lambda::deploy_lambda(\n  tag = \"my-lambda\",\n  runtime_function = \"my_fun\",\n  runtime_path = \"path/to/script/of/my_fun\",\n  dependencies = c(\"ggplot2\", \"dplyr\")\n  )\n  \nIn my first post on the topic, I noted that this very much a work in progress, and that I hope to actively develop this package by adding features that would make it useful in more realistic scenarios.\n\nTesting R-based AWS Lambda functions locally\nOne such feature was the ability to test our R-based Lambda function locally before deploying to the AWS cloud. This is super useful, because, depending on the size of the docker image it might take a while to push it to the AWS ECR repository. Also, creating an AWS Lambda function from the ECR docker image, requires granting the function a role and permissions policy to execute or access other services. All of these steps create resources/services in your AWS account, so ideally, would only be done when we are certain that docker image we are deploying as a Lambda function works correctly.\nThis procedure is well documented. After creating the Lambda docker image, to test it locally, we need to 1) run a container on our local machine and 2) send a request to it with curl. Essentially, we are ‘invoking’ the function with the same payload the same way we’d do it in the cloud, but locally. This is the best way to know that everything works as intended. Merely testing the R code separately, without the Lambda docker context, might not be enough.\nI packaged this routine in the function test_lambda and added this step to the {r2lamdba} deployment workflow. With these changes, instead of one function deploy_lambda that would build and deploy the image, we now have thee steps:\n\nbuild_lambda – to build and tag the docker image locally\ntest_lambda – to test the lambda docker container locally (optional but recommended)\ndeploy_lambda – to push the docker image to the cloud and create the function\n\nOr in code:\n\nBuild a docker image for the lambda function\nruntime_function &lt;- \"parity\"\nruntime_path &lt;- system.file(\"parity.R\", package = \"r2lambda\")\ndependencies &lt;- NULL\n\n# Might take a while, its building a docker image\nbuild_lambda(\n tag = \"parity1\",\n runtime_function = runtime_function,\n runtime_path = runtime_path,\n dependencies = dependencies\n )\n\n\nTest the lambda docker image locally\npayload &lt;- list(number = 2)\ntag &lt;- \"parity1\"\ntest_lambda(tag = \"parity1\", payload)\n\n\nDeploy to AWS Lambda\n# Might take a while, its pushing it to a remote repository\ndeploy_lambda(tag = \"parity1\")\n\n\nInvoke deployed lambda\ninvoke_lambda(\n  function_name = \"parity1\",\n  invocation_type = \"RequestResponse\",\n  payload = list(number = 2),\n  include_logs = FALSE\n)\n\n#&gt; Lambda response payload: \n#&gt; {\"parity\":\"even\"}\nSo, although we’ve added a couple of steps to the workflow, I think its for the better, as we can have finer control over building and deploying. For example, sometimes we might want to deploy a Lambda function from an existing docker image, so de-coupling the build and deploy steps makes a lot of sense.\nWould love to hear from you! Let me know if you try the r2lamdba package or if you know of any similar projects."
  },
  {
    "objectID": "posts/r2lambda/2023-03-09-lambda-to-s3/index.html",
    "href": "posts/r2lambda/2023-03-09-lambda-to-s3/index.html",
    "title": "How to set up an R-based AWS Lambda to write to AWS S3 on a schedule",
    "section": "",
    "text": "At the end of this tutorial, we would have created an AWS Lambda function that fetches the most-recent Tidytuesday dataset and writes it into an S3 Bucket every Wednesday. To do this, we’ll first work interactively with {r2lambda} and {paws} to go through all the steps the Lambda function would eventually need to do, then wrap the code and deploy it to AWS Lambda, and finally schedule it to run weekly."
  },
  {
    "objectID": "posts/r2lambda/2023-03-09-lambda-to-s3/index.html#overview",
    "href": "posts/r2lambda/2023-03-09-lambda-to-s3/index.html#overview",
    "title": "How to set up an R-based AWS Lambda to write to AWS S3 on a schedule",
    "section": "",
    "text": "At the end of this tutorial, we would have created an AWS Lambda function that fetches the most-recent Tidytuesday dataset and writes it into an S3 Bucket every Wednesday. To do this, we’ll first work interactively with {r2lambda} and {paws} to go through all the steps the Lambda function would eventually need to do, then wrap the code and deploy it to AWS Lambda, and finally schedule it to run weekly."
  },
  {
    "objectID": "posts/r2lambda/2023-03-09-lambda-to-s3/index.html#getting-started-with-aws-simple-storage-service-s3-from-r",
    "href": "posts/r2lambda/2023-03-09-lambda-to-s3/index.html#getting-started-with-aws-simple-storage-service-s3-from-r",
    "title": "How to set up an R-based AWS Lambda to write to AWS S3 on a schedule",
    "section": "Getting started with AWS Simple Storage Service (S3) from R",
    "text": "Getting started with AWS Simple Storage Service (S3) from R\n\nlibrary(r2lambda)\nlibrary(tidytuesdayR)\n\nAs with any AWS service supported by {paws}, we can easily connect to S3 and perform some basic operations. Below, we establish an S3 service using r2lambda::aws_connect, then create a bucket called tidytuesday-dataset, drop and then delete and empty file, and delete the bucket altogether. This exercise is not very meaningful beyond learning the basics on how to interact with S3 from R. Eventually, though, our lambda function would need to do something similar, so being familiar with the process in an interactive session helps.\nTo run any of the code below, you need some environmental variables set. See the Setup section in the {r2lambda} package readme for more details\n\ns3_service &lt;- aws_connect(\"s3\")\n\n# create a bucket on S3\ns3_service$create_bucket(Bucket = \"a-unique-bucket\")\n\n# upload an object to our bucket\ntmpfile &lt;- tempfile(pattern = \"object_\", fileext = \"txt\")\nwrite(\"test\", tmpfile)\n(readLines(tmpfile))\ns3_service$put_object(Body = tmpfile, Bucket = \"a-unique-bucket\", Key = \"TestFile\")\n\n# list the contents of a bucket\ns3_service$list_objects(Bucket = \"a-unique-bucket\")\n\n# delete an object from a bucket\ns3_service$delete_object(Bucket = \"a-unique-bucket\", Key = \"TestFile\")\n\n# delete a bucket\ns3_service$delete_bucket(Bucket = \"a-unique-bucket\")\n\nNow, the above procedure used a local file, but what if we generated some data during our session, and we want to stream that directly to S3 without saving to file? In many cases, we don’t have the option to write to disk or simply don’t want to.\nIn such cases we need to serialize our data object before trying to put it in the bucket. This comes down to calling serialize with connection=NULL to generate a raw vector without writing to a file. We can then put the iris data set from memory into our a-unique-bucket S3 bucket.\n\ns3_service &lt;- aws_connect(\"s3\")\n\n# create a bucket on S3\ns3_service$create_bucket(Bucket = \"a-unique-bucket\")\n\n# upload an object to our bucket\nsiris &lt;- serialize(iris, connection = NULL)\ns3_service$put_object(Body = siris, Bucket = \"a-unique-bucket\", Key = \"TestFile2\")\n\n# list the contents of a bucket\ns3_service$list_objects(Bucket = \"a-unique-bucket\")\n\n# delete an object from a bucket\ns3_service$delete_object(Bucket = \"a-unique-bucket\", Key = \"TestFile2\")\n\n# delete a bucket\ns3_service$delete_bucket(Bucket = \"a-unique-bucket\")\n\nOK. With that, we now know the two steps our Lambda function would need to do:\n\nfetch the most recent Tidytuesday data set (see this post for details)\nput the data set as an object in the S3 bucket\n\nStill in an interactive session, lets just write the code that our Lambda would have to execute.\n\nlibrary(tidytuesdayR)\n\n# Find the most recent tuesday and fetch the corresponding data set\nmost_recent_tuesday &lt;- tidytuesdayR::last_tuesday(date = Sys.Date())\ntt_data &lt;- tidytuesdayR::tt_load(x = most_recent_tuesday)\n\n# by default it comes as class `tt_data`, which causes problems\n# with serialization and conversion to JSON. So best to extract\n# the data set(s) as a simple list\ntt_data &lt;- lapply(names(tt_data), function(x) tt_data[[x]])\n\n# then serialize\ntt_data_raw &lt;- serialize(tt_data, connection = NULL)\n\n# create a bucket on S3\ns3_service &lt;- r2lambda::aws_connect(\"s3\")\ns3_service$create_bucket(Bucket = \"tidytuesday-datasets\")\n\n# upload an object to our bucket\ns3_service$put_object(\n  Body = tt_data_raw, \n  Bucket = \"tidytuesday-datasets\", \n  Key = most_recent_tuesday\n)\n\n# list the contents of our bucket and find the Keys for all objects\nobjects &lt;- s3_service$list_objects(Bucket = \"tidytuesday-datasets\")\nsapply(objects$Contents, \"[[\", \"Key\")\n#&gt; [1] \"2023-03-07\"\n\n# fetch a Tidytuesday dataset from S3\ntt_dataset &lt;- s3_service$get_object(\n  Bucket = \"tidytuesday-datasets\", \n  Key = most_recent_tuesday\n)\n\n# convert from raw and show the first few rows\ntt_dataset$Body %&gt;% unserialize() %&gt;% head()\n\nNow we should have everything we need to write our Lambda function."
  },
  {
    "objectID": "posts/r2lambda/2023-03-09-lambda-to-s3/index.html#lambda-s3-integration-dropping-a-file-in-an-s3-bucket",
    "href": "posts/r2lambda/2023-03-09-lambda-to-s3/index.html#lambda-s3-integration-dropping-a-file-in-an-s3-bucket",
    "title": "How to set up an R-based AWS Lambda to write to AWS S3 on a schedule",
    "section": "Lambda + S3 integration: Dropping a file in an S3 bucket",
    "text": "Lambda + S3 integration: Dropping a file in an S3 bucket\nWrapping the above interactive code into a function and also, defining an s3_connect function as a helper to create an S3 client within the function. By doing this, we avoid adding r2lambda as a dependency to the Lambda function. (At the time of writing, r2lambda does not yet support non-CRAN packages.)\n\ns3_connect &lt;- function() {\n  paws::s3(config = list(\n    credentials = list(\n      creds = list(\n        access_key_id = Sys.getenv(\"ACCESS_KEY_ID\"),\n        secret_access_key = Sys.getenv(\"SECRET_ACCESS_KEY\")\n      ),\n      profile = Sys.getenv(\"PROFILE\")\n    ),\n    region = Sys.getenv(\"REGION\")\n  ))\n}\n\ntidytuesday_lambda_s3 &lt;- function() {\n  most_recent_tuesday &lt;- tidytuesdayR::last_tuesday(date = Sys.Date())\n  tt_data &lt;- tidytuesdayR::tt_load(x = most_recent_tuesday)\n  tt_data &lt;- lapply(names(tt_data), function(x) tt_data[[x]])\n  tt_data_raw &lt;- serialize(tt_data, connection = NULL)\n  \n  s3_service &lt;- s3_connect()\n  s3_service$put_object(Body = tt_data_raw,\n                        Bucket = \"tidytuesday-datasets\",\n                        Key = most_recent_tuesday)\n  \n}\n\nNow, calling tidytuesday_lambda_s3() should fetch and put the most recent Tidytuesday data set into our S3 bucket. To test it, we run:\n\ntidytuesday_lambda_s3()\n\nlist_objects &lt;- function(bucket) {\n  s3 &lt;- s3_connect()\n  obj &lt;- s3$list_objects(Bucket = bucket)\n  sapply(obj$Contents, \"[[\", \"Key\")\n}\n\nlist_objects(\"tidytuesday-datasets\")\n#&gt; [1] \"2023-03-07\"\n\nOn to the next step, to create and deploy the Lambda function. We have a few considerations here:\n\nFor the Lambda function to connect to S3, it needs access to some environmental variables. The same ones as we have in our current interactive session without which we can’t establish local clients of AWS services. These are: REGION, PROFILE, SECRET_ACCESS_KEY, and ACCESS_KEY_ID. To include these envvars in the Lambda docker image on deploy, use the set_aws_envvars argument of deploy_lambda.\nWe have some dependencies that would need to be available in the docker image. We already saw how to install {tidytuesdayR} in our Lambda docker image in a previous post. Besides this, we also need to install {paws}, because without it we can’t interact with S3. To do this, we just need to add dependencies = c(\"tidytuesdayR\", \"paws\") when building the image with r2lambda::build_lambda.\n\n\nBuild\n\nr_code &lt;- \"\n  s3_connect &lt;- function() {\n    paws::s3(config = list(\n      credentials = list(\n        creds = list(\n          access_key_id = Sys.getenv('ACCESS_KEY_ID'),\n          secret_access_key = Sys.getenv('SECRET_ACCESS_KEY')\n        ),\n        profile = Sys.getenv('PROFILE')\n      ),\n      region = Sys.getenv('REGION')\n    ))\n  }\n  \n  tidytuesday_lambda_s3 &lt;- function() {\n    most_recent_tuesday &lt;- tidytuesdayR::last_tuesday(date = Sys.Date())\n    tt_data &lt;- tidytuesdayR::tt_load(x = most_recent_tuesday)\n    tt_data &lt;- lapply(names(tt_data), function(x) tt_data[[x]])\n    tt_data_raw &lt;- serialize(tt_data, connection = NULL)\n    \n    s3_service &lt;- s3_connect()\n    s3_service$put_object(Body = tt_data_raw,\n                          Bucket = 'tidytuesday-datasets',\n                          Key = most_recent_tuesday)\n  }\n  \n  lambdr::start_lambda()\n\"\n\ntmpfile &lt;- tempfile(pattern = \"tt_lambda_s3_\", fileext = \".R\")\nwrite(x = r_code, file = tmpfile)\n\n\nruntime_function &lt;- \"tidytuesday_lambda_s3\"\nruntime_path &lt;- tmpfile\ndependencies &lt;- c(\"tidytuesdayR\", \"paws\")\n\nr2lambda::build_lambda(\n  tag = \"tidytuesday_lambda_s3\",\n  runtime_function = runtime_function,\n  runtime_path = runtime_path,\n  dependencies = dependencies\n)\n\n\n\nDeploy\nWe set a generous 2 minute timeout, just to be safe that the data set is successfully copied to S3. And we also increase the available memory to 1024 mb. Note also the flag to pass along our local AWS envvars to the deployed lambda environment.\n\nr2lambda::deploy_lambda(\n  tag = \"tidytuesday_lambda_s3\",\n  set_aws_envvars = TRUE,\n  Timeout = 120,\n  MemorySize = 1024)\n\n\n\nInvoke\nWe invoke as usual, with an empty list as payload because our function does not take any arguments.\n\nr2lambda::invoke_lambda(\n  function_name = \"tidytuesday_lambda_s3\", \n  invocation_type = \"RequestResponse\", \n  payload = list(),\n  include_logs = TRUE)\n\n#&gt; INFO [2023-03-08 23:50:46] [invoke_lambda] Validating inputs.\n#&gt; INFO [2023-03-08 23:50:46] [invoke_lambda] Checking function state.\n#&gt; INFO [2023-03-08 23:50:47] [invoke_lambda] Function state: Active.\n#&gt; INFO [2023-03-08 23:50:47] [invoke_lambda] Invoking function.\n#&gt; \n#&gt; Lambda response payload: \n#&gt; {\"Expiration\":[],\"ETag\":\"\\\"4f5a6085215b9074faed28d816696a99\\\"\",\"ChecksumCRC32\":[],\n#&gt; \"ChecksumCRC32C\":[],\"ChecksumSHA1\":[],\"ChecksumSHA256\":[],\"ServerSideEncryption\":\"AES256\",\n#&gt; \"VersionId\":[],\"SSECustomerAlgorithm\":[],\"SSECustomerKeyMD5\":[],\"SSEKMSKeyId\":[],\n#&gt; \"SSEKMSEncryptionContext\":[],\"BucketKeyEnabled\":[],\"RequestCharged\":[]}\n#&gt; \n#&gt; Lambda logs: \n#&gt; OpenBLAS WARNING - could not determine the L2 cache size on this system, assuming 256k\n#&gt; INFO [2023-03-09 05:50:49] Using handler function  tidytuesday_lambda_s3\n#&gt; START RequestId: c6cb0600-3400-4ca3-9232-8af53542f8e8 Version: $LATEST\n#&gt; --- Compiling #TidyTuesday Information for 2023-03-07 ----\n#&gt; --- There is 1 file available ---\n#&gt; --- Starting Download ---\n#&gt; Downloading file 1 of 1: `numbats.csv`\n#&gt; --- Download complete ---\n#&gt; END RequestId: c6cb0600-3400-4ca3-9232-8af53542f8e8\n#&gt; REPORT RequestId: c6cb0600-3400-4ca3-9232-8af53542f8e8   Duration: 12061.06 ms   \n#&gt; Billed Duration: 13331 ms    Memory Size: 1024 MB    Max Memory Used: 181 MB Init \n#&gt; Duration: 1269.59 ms \n#&gt; SUCCESS [2023-03-08 23:51:01] [invoke_lambda] Done.\n\nThen, to confirm that a Tidytuesday data set was written to S3 as an object in the bucket tidytuesday-datasets we would run:\n\ns3_service &lt;- r2lambda::aws_connect(service = \"s3\")\nobjs &lt;- s3_service$list_objects(Bucket = \"tidytuesday-datasets\")\nobjs$Contents[[1]]$Key\n#&gt; [1] \"2023-03-07\"\n\nWe expect to see one object with a Key matching the date of the most recent Tuesday. At the time of writing that is March 7, 2023.\n\n\nSchedule\nFinally, to copy the Tidytuesday dataset on a weekly basis, for example, every Wednesday, we would use r2lambda::schedule_lambda with an execution rate set by cron.\nFirst, to validate that things are working, we can set the lambda on a 5-minute schedule and check the time stamp on the on the S3 object to make sure it is updated every 5 minutes:\n\n# schedule the lambda to execute every 5 minutes\nr2lambda::schedule_lambda(\n  lambda_function = \"tidytuesday_lambda_s3\", \n  execution_rate = \"rate(5 minutes)\"\n  )\n\n# occasionally query the S3 bucket status and the LastModified time stamp\nobjs &lt;- s3_service$list_objects(Bucket = \"tidytuesday-datasets\")\nobjs$Contents[[1]]$LastModified\n\nIf all is well, set it to run every Wednesday at midnight:\n\nr2lambda::schedule_lambda(\n  lambda_function = \"tidytuesday_lambda_s3\",\n  execution_rate = \"cron(0 0 * * Wed *)\"\n  )\n\nNext Wednesday morning, we should have two objects, with keys matching the two most-recent Tuesdays."
  },
  {
    "objectID": "posts/r2lambda/2023-03-09-lambda-to-s3/index.html#summary",
    "href": "posts/r2lambda/2023-03-09-lambda-to-s3/index.html#summary",
    "title": "How to set up an R-based AWS Lambda to write to AWS S3 on a schedule",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "posts/r2lambda/2022-07-24-invoke-an-aws-lambda-function-with-r-and-paws/index.html",
    "href": "posts/r2lambda/2022-07-24-invoke-an-aws-lambda-function-with-r-and-paws/index.html",
    "title": "How to invoke an AWS Lambda function with R and paws",
    "section": "",
    "text": "In recent weeks, I have been trying to learn more about using the {paws} software development kit to interact with Amazon Web Services from R. So far, my focus was on the basics of DynamoDB. How to put one item in the database, and how to migrate reasonably sized table into DynamoDB. There are more topics on DynamoDB that I would focus on, and I hope to document my experience in future posts. But today, I wanted to switch gears to AWS Lambda. Specifically, how to invoke an already deployed cloud function from R."
  },
  {
    "objectID": "posts/r2lambda/2022-07-24-invoke-an-aws-lambda-function-with-r-and-paws/index.html#aws-setup",
    "href": "posts/r2lambda/2022-07-24-invoke-an-aws-lambda-function-with-r-and-paws/index.html#aws-setup",
    "title": "How to invoke an AWS Lambda function with R and paws",
    "section": "AWS Setup",
    "text": "AWS Setup\nIn my R+AWS projects I typically include an .Renviron file that stores the needed settings and secrets for authenticating with AWS. These environmental variables are passed as config when starting services with paws. A minimal .Renviron for this purpose might look like so:\n\nACCESS_KEY_ID = \"MYKEY\"\nSECRET_ACCESS_KEY = \"MYSECRET\"\nPROFILE = \"default\"\nREGION = \"us-east-1\"\n\nThen, within our R session, we would get these configurations with the typical:\n\nSys.getenv(\"REGION\")\nSys.getenv(\"PROFILE\")\n\nAlternatively, if we can’t or don’t want to store secrets in a file, we could set them directly in the code:\n\nSys.setenv(REGION = 'us-east-2')"
  },
  {
    "objectID": "posts/r2lambda/2022-07-24-invoke-an-aws-lambda-function-with-r-and-paws/index.html#starting-an-aws-lambda-service-in-r",
    "href": "posts/r2lambda/2022-07-24-invoke-an-aws-lambda-function-with-r-and-paws/index.html#starting-an-aws-lambda-service-in-r",
    "title": "How to invoke an AWS Lambda function with R and paws",
    "section": "Starting an AWS Lambda service in R",
    "text": "Starting an AWS Lambda service in R\nNow, with our configuration prerequisites in place, we can establish a local lambda service using paws::lambda.\n\nlambda_service &lt;- paws::lambda(config = list(\n  credentials = list(\n    creds = list(\n      access_key_id = Sys.getenv(\"ACCESS_KEY_ID\"),\n      secret_access_key = Sys.getenv(\"SECRET_ACCESS_KEY\")\n    ),\n    profile = Sys.getenv(\"PROFILE\")\n  ),\n  region = Sys.getenv(\"REGION\")\n))\n\nWith the service started, we can see the functions available in our AWS cloud, and the operations we can perform.\n\n&gt; all_lambdas &lt;- lambda_service$list_functions()\n&gt; sapply(all_lambdas$Functions, \"[[\" , \"FunctionName\")\n[1] \"diamonds\"     \"parity\"       \"r-lambda-poc\"\n\nCurrently, I have three deployed Lambda functions, two of which are examples from David Neuzerling’s excellent work with the {lambdr} R package. For more details on these (diamonds and parity), see David’s writing on the topic here, here, and here, which was and still is incredibly helpful for me to get started and keep learning."
  },
  {
    "objectID": "posts/r2lambda/2022-07-24-invoke-an-aws-lambda-function-with-r-and-paws/index.html#invoking-an-aws-lambda-function-with-r-and-paws",
    "href": "posts/r2lambda/2022-07-24-invoke-an-aws-lambda-function-with-r-and-paws/index.html#invoking-an-aws-lambda-function-with-r-and-paws",
    "title": "How to invoke an AWS Lambda function with R and paws",
    "section": "Invoking an AWS lambda function with R and {paws}",
    "text": "Invoking an AWS lambda function with R and {paws}\nAs an example, I am going to use the parity lambda, described in detail in one of the vignettes of the {lambdr} package. The input (payload) of this lambda function is a named list of the form list(number = 2), which in JSON becomes:\n\n# parity lambda input payload\n{\"number\": 2}\n\nThe return is also a one-item named list with JSON {\"parity\": \"odd\"|\"even\"}. The R function run to assess the parity is the following:\n\nparity &lt;- function(number) {\n  list(parity = if (as.integer(number) %% 2 == 0) \"even\" else \"odd\")\n}\n\nTo invoke this function we need to call the invoke operation of our lambda service, and supply it with 1) the name of the function (or function arn), 2) the type of invocation (‘DryRun’, ‘RequestResponse’, or ‘Event’ see ?paws.compute::lambda_invoke), and 3) the input payload (in this case '{ \"number\": \"2\" }'). We’ll also ask for the tail of the execution log so we can get more information regarding the duration, memory usage, and billing information.\n\n# invoke the parity lambda\nresponse &lt;- lambda_service$invoke(\n  FunctionName = \"parity\",\n  InvocationType = \"RequestResponse\",\n  Payload = '{ \"number\": \"2\" }',\n  LogType = \"Tail\"\n)\n\nAfter a second or two, we can decode the output payload, we are expecting parity = \"even\":\n\n&gt; response$Payload |&gt; rawToChar() |&gt; cat()\n{\"parity\":\"even\"}\n\nFor the logs, we should first decode the response string, as it comes base64 encoded. And then, convert to a human-readable character vector:\n\n&gt; jsonlite::base64_dec(response$LogResult) |&gt; rawToChar() |&gt; cat()\nSTART RequestId: 7c971dd1-938d-4ae7-ad35-e1ee5b59e6bb Version: $LATEST\nEND RequestId: 7c971dd1-938d-4ae7-ad35-e1ee5b59e6bb\nREPORT RequestId: 7c971dd1-938d-4ae7-ad35-e1ee5b59e6bb  Duration: 778.92 ms Billed Duration: 3002 ms    Memory Size: 128 MB Max Memory Used: 98 MB  Init Duration: 2222.94 ms\n\n(Using cat() here because it handles newlines nicely)"
  },
  {
    "objectID": "posts/r2lambda/2022-07-24-invoke-an-aws-lambda-function-with-r-and-paws/index.html#using-paws-bulding-blocks-to-build-a-custom-aws-lambda-invocation-function",
    "href": "posts/r2lambda/2022-07-24-invoke-an-aws-lambda-function-with-r-and-paws/index.html#using-paws-bulding-blocks-to-build-a-custom-aws-lambda-invocation-function",
    "title": "How to invoke an AWS Lambda function with R and paws",
    "section": "Using paws bulding blocks to build a custom AWS Lambda invocation function",
    "text": "Using paws bulding blocks to build a custom AWS Lambda invocation function\nFirst, we can wrap the call to paws::lambda in a function so we can use it to connect to different AWS services. For now, we assume the environmental variables are set, but we would implement checks and error handling in production setting.\n\naws_connect &lt;- function(service) {\n  service(config = list(\n    credentials = list(\n      creds = list(\n        access_key_id = Sys.getenv(\"ACCESS_KEY_ID\"),\n        secret_access_key = Sys.getenv(\"SECRET_ACCESS_KEY\")\n      ),\n      profile = Sys.getenv(\"PROFILE\")\n    ),\n    region = Sys.getenv(\"REGION\")\n  ))\n}\n\nWith this function, we can now quickly create a dynamoDB and Lambda services:\n\nlambda_service &lt;- aws_connect(paws::lambda)\nlambda_service$list_functions()\n# output truncated\n\ndynamodb_service &lt;- aws_connect(paws::dynamodb)\ndynamodb_service$list_tables()\n# output truncated\n\nThen, we go to the next step, to wrap the service start and invocation in one function. The payload is a named list, which internally is converted to JSON by jsonlite::toJSON.\n\ninvoke_lambda &lt;-\n  function(function_name,\n           invocation_type,\n           payload,\n           include_logs = FALSE) {\n    # assumes .Renviron is set up\n    lambda_service &lt;- aws_connect(paws::lambda)\n    \n    response &lt;- lambda_service$invoke(\n      FunctionName = function_name,\n      InvocationType = invocation_type,\n      Payload = jsonlite::toJSON(payload),\n      LogType = ifelse(include_logs, \"Tail\", \"None\")\n    )\n    \n    message(\"\\nLambda response payload: \")\n    response$Payload |&gt; rawToChar() |&gt; cat()\n    \n    if (include_logs) {\n      message(\"\\nLambda logs: \")\n      jsonlite::base64_dec(response$LogResult) |&gt; rawToChar() |&gt; cat()\n    }\n    \n    invisible(response)\n  }\n\nFinally, we run our simple helpful wrapper:\n\n&gt; invoke_lambda(\n+   function_name = \"parity\",\n+   invocation_type = \"RequestResponse\",\n+   payload = list(number = 5),\n+   include_logs = FALSE\n+ )\n\nLambda response payload: \n{\"parity\":\"odd\"}\n\n&gt; invoke_lambda(\n+   function_name = \"parity\",\n+   invocation_type = \"RequestResponse\",\n+   payload = list(number = 5),\n+   include_logs = TRUE\n+ )\n\nLambda response payload: \n{\"parity\":\"odd\"}\nLambda logs: \nSTART RequestId: b3507d9d-1218-4c1a-8a2a-d27c728b5093 Version: $LATEST\nEND RequestId: b3507d9d-1218-4c1a-8a2a-d27c728b5093\nREPORT RequestId: b3507d9d-1218-4c1a-8a2a-d27c728b5093  Duration: 134.67 ms Billed Duration: 135 ms Memory Size: 128 MB Max Memory Used: 101 MB"
  },
  {
    "objectID": "posts/r2lambda/2022-07-24-invoke-an-aws-lambda-function-with-r-and-paws/index.html#summary",
    "href": "posts/r2lambda/2022-07-24-invoke-an-aws-lambda-function-with-r-and-paws/index.html#summary",
    "title": "How to invoke an AWS Lambda function with R and paws",
    "section": "Summary",
    "text": "Summary\nThese were some baby steps in accessing AWS Lambda services and invoking functions from R using paws. There is a lot more to be done with paws and lambda, including creating lambda functions, linking them to other AWS services, using them in {shiny} applications, etc. I hope to cover some of these in future posts."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/r2lambda/2023-02-24-an-r-aws-lambda-function-to-download-tidytuesday-datasets/index.html",
    "href": "posts/r2lambda/2023-02-24-an-r-aws-lambda-function-to-download-tidytuesday-datasets/index.html",
    "title": "An R AWS Lambda function to download Tidytuesday datasets",
    "section": "",
    "text": "In this exercise, we’ll create an AWS Lambda function that downloads the tidytuesday data set for the most recent Tuesday (or most recent Tuesday from a date of interest)."
  },
  {
    "objectID": "posts/r2lambda/2023-02-24-an-r-aws-lambda-function-to-download-tidytuesday-datasets/index.html#use-r2lambda-to-download-tidytuesday-dataset",
    "href": "posts/r2lambda/2023-02-24-an-r-aws-lambda-function-to-download-tidytuesday-datasets/index.html#use-r2lambda-to-download-tidytuesday-dataset",
    "title": "An R AWS Lambda function to download Tidytuesday datasets",
    "section": "",
    "text": "In this exercise, we’ll create an AWS Lambda function that downloads the tidytuesday data set for the most recent Tuesday (or most recent Tuesday from a date of interest)."
  },
  {
    "objectID": "posts/r2lambda/2023-02-24-an-r-aws-lambda-function-to-download-tidytuesday-datasets/index.html#required-packages",
    "href": "posts/r2lambda/2023-02-24-an-r-aws-lambda-function-to-download-tidytuesday-datasets/index.html#required-packages",
    "title": "An R AWS Lambda function to download Tidytuesday datasets",
    "section": "Required packages",
    "text": "Required packages\n\nlibrary(r2lambda)\nlibrary(jsonlite)\nlibrary(magrittr)"
  },
  {
    "objectID": "posts/r2lambda/2023-02-24-an-r-aws-lambda-function-to-download-tidytuesday-datasets/index.html#runtime-function",
    "href": "posts/r2lambda/2023-02-24-an-r-aws-lambda-function-to-download-tidytuesday-datasets/index.html#runtime-function",
    "title": "An R AWS Lambda function to download Tidytuesday datasets",
    "section": "Runtime function",
    "text": "Runtime function\nThe first step is to write the runtime function. This is the function that will be executed when we invoke the Lambda function after it has been deployed. To download the Tidytuesday data set, we will use the {tidytuesdayR} package. In the runtime script, we define a function called tidytyesday_lambda that takes one optional argument date. If date is omitted, the function returns the data set(s) for the most recent Tuesday, otherwise, it looks up the most recent Tuesday from a date of interest and returns the corresponding data set(s).\n\nlibrary(tidytuesdayR)\n\ntidytuesday_lambda &lt;- function(date = NULL) {\n  if (is.null(date))\n    date &lt;- Sys.Date()\n  \n  most_recent_tuesday &lt;- tidytuesdayR::last_tuesday(date = date)\n  tt_data &lt;- tidytuesdayR::tt_load(x = most_recent_tuesday)\n  data_names &lt;- names(tt_data)\n  data_list &lt;- lapply(data_names, function(x) tt_data[[x]])\n  return(data_list)\n}\n\ntidytuesday_lambda(\"2022-02-02\")"
  },
  {
    "objectID": "posts/r2lambda/2023-02-24-an-r-aws-lambda-function-to-download-tidytuesday-datasets/index.html#r-script-to-build-the-lambda",
    "href": "posts/r2lambda/2023-02-24-an-r-aws-lambda-function-to-download-tidytuesday-datasets/index.html#r-script-to-build-the-lambda",
    "title": "An R AWS Lambda function to download Tidytuesday datasets",
    "section": "R script to build the lambda",
    "text": "R script to build the lambda\nTo build the lambda image, we need an R script that sources any required code, loads any needed libraries, defines a runtime function, and ends with a call to lambdr::start_lambda(). The runtime function does not have to be defined in this file. We could, for example, source another script, or load a package and set a loaded function as the runtime function in the subsequent call to r2lambda::build_lambda (see below). We save this script to a file and record the path:\n\nr_code &lt;- \"\n  library(tidytuesdayR)\n\n  tidytuesday_lambda &lt;- function(date = NULL) {\n    if (is.null(date))\n      date &lt;- Sys.Date()\n    \n    most_recent_tuesday &lt;- tidytuesdayR::last_tuesday(date = date)\n    tt_data &lt;- tidytuesdayR::tt_load(x = most_recent_tuesday)\n    data_names &lt;- names(tt_data)\n    data_list &lt;- lapply(data_names, function(x) tt_data[[x]])\n    return(data_list)\n  }\n  \n  lambdr::start_lambda()\n\"\n\ntmpfile &lt;- tempfile(pattern = \"ttlambda_\", fileext = \".R\")\nwrite(x = r_code, file = tmpfile)"
  },
  {
    "objectID": "posts/r2lambda/2023-02-24-an-r-aws-lambda-function-to-download-tidytuesday-datasets/index.html#build-test-and-deploy-the-lambda-function",
    "href": "posts/r2lambda/2023-02-24-an-r-aws-lambda-function-to-download-tidytuesday-datasets/index.html#build-test-and-deploy-the-lambda-function",
    "title": "An R AWS Lambda function to download Tidytuesday datasets",
    "section": "Build, test, and deploy the lambda function",
    "text": "Build, test, and deploy the lambda function\n\n1. Build\n\nWe set the runtime_function argument to the name of the function we wish the docker container to run when invoked. In this case, this is tidytuesday_lambda. This adds a CMD instruction to the Dockerfile\nWe set the runtime_path argument to the path we stored the script defining our runtime function.\nWe set the dependencies argument to c(\"tidytuesdayR\")because we need to have the tidytuesdayR package installed within the docker container if we are to download the dataset. This steps adds a RUN instruction to the Dockerfile that calls install.packages to install {tidytuesdayR} from CRAN.\nFinally, the tag argument sets the name of our Lambda function which we’ll use later to test and invoke the function. The tag argument also becomes the name of the folder that {r2lambda} will create to build the image. This folder will have two files, Dockerfile and runtime.R. runtime.R is our script from runtime_path, renamed before it is copied in the docker image with a COPY instruction.\n\n\nruntime_function &lt;- \"tidytuesday_lambda\"\nruntime_path &lt;- tmpfile\ndependencies &lt;- \"tidytuesdayR\"\n\nr2lambda::build_lambda(\n  tag = \"tidytuesday3\",\n  runtime_function = runtime_function,\n  runtime_path = runtime_path,\n  dependencies = dependencies\n)\n\n\n\n2. Test\nTo make sure our Lambda docker container works as intended, we start it locally, and invoke it to test the response. The response is a list of three elements:\n\nresponse &lt;- r2lambda::test_lambda(tag = \"tidytuesday3\", payload = list(date = Sys.Date()))\n\n\nstatus, should be 0 if the test worked,\nstdout, the standard output stream of the invocation, and\nstderr, the standard error stream of the invocation\n\nstdout and stderr are raw vectors that we need to parse, for example:\n\nrawToChar(response$stdout) \n\nIf the stdout slot of the response returns the correct output of our function, we are good to deploy to AWS.\n\n\n3. Deploy\nThe deployment step is simple, in that all we need to do is specify the name (tag) of the Lambda function we wish to push to AWS ECR. The deploy_lambda function also accepts ..., which are named arguments ultimately passed onto paws.compute:::lambda_create_function. This is the function that calls the Lambda API. To see all available arguments run ?paws.compute:::lambda_create_function.\nThe most important arguments are probably Timeout and MemorySize, which set the time our function will be allowed to run and the amount of memory it will have available. In many cases it will make sense to increase the defaults of 3 seconds and 128 mb.\n\nr2lambda::deploy_lambda(tag = \"tidytuesday3\", Timeout = 30)\n\n\n\n4. Invoke\nIf all goes well, our function should now be available on the cloud awaiting requests. We can invoke it from R using invoke_lambda. The arguments are:\n\nfunction_name – the name of the function\ninvocation_type – typically RequestResponse\ninclude_log – whether to print the logs of the run on the console\npayload – a named list with arguments sent to the runtime_function. In this case, the runtime function, tidytuesday_lambda has a single argument date, so the corresponding list is list(date = Sys.Date()). As our function can be called without any argument, we can also send an empty list as the payload.\n\n\nresponse &lt;- r2lambda::invoke_lambda(\n  function_name = \"tidytuesday3\",\n  invocation_type = \"RequestResponse\",\n  payload = list(),\n  include_logs = TRUE\n)\n\nJust like in the local test, the response payload comes as a raw vector that needs to be parsed into a data.frame:\n\ntidytuesday_dataset &lt;- response$Payload %&gt;% \n  rawToChar() %&gt;% \n  jsonlite::fromJSON(simplifyDataFrame = TRUE)\n\ntidytuesday_dataset[[1]][1:5, 1:5]"
  },
  {
    "objectID": "posts/r2lambda/2023-02-24-an-r-aws-lambda-function-to-download-tidytuesday-datasets/index.html#summary",
    "href": "posts/r2lambda/2023-02-24-an-r-aws-lambda-function-to-download-tidytuesday-datasets/index.html#summary",
    "title": "An R AWS Lambda function to download Tidytuesday datasets",
    "section": "Summary",
    "text": "Summary\nIn this post, we went over some details about:\n\nhow to prepare an R script before deploying it as a Lambda function,\n\nwhat are the roles of several of the key arguments,\n\nhow to request longer timeout or more memory for a Lambda function, and\n\nhow to parse the response payload returned by the Lambda function\n\nStay tuned for a follow-up post where we set this Lambda function to run on a weekly schedule!"
  },
  {
    "objectID": "posts/r2lambda/2024-10-17-r2lambda-renv/index.html",
    "href": "posts/r2lambda/2024-10-17-r2lambda-renv/index.html",
    "title": "r2lambda update to support multi-file projects and renv projects",
    "section": "",
    "text": "It has been a while since I’ve had the chance to work on my {r2lambda} project. In particular, there were a couple of good points made by a user on GitHub about functionality that is missing from the package. The option to deploy multiple files, e.g., one runtime function that depends on helpers in the same project organized in different files. And another, to enable {renv} management of the R environment within the AWS Lambda docker image. Both excellent points that I wished I addressed earlier. But better late than never.\nBoth of these features required minor adjustments to the codebase. Copying additional supports scripts and restoring the {renv} environment should both happen when the AWS Lambda docker image is built, so the logic to create the Dockerfile needed to be updated. Accordinly, the r2lambda::build_lambda function now has two additional arguments:\n#' @param support_path path to the support files (if any). Either NULL \n#' (the default) if all needed code is in the same `runtime_path` script, or a \n#' character vector of paths to additional files needed by the runtime script.\n#' @param renvlock_path path to the renv.lock file (if any). Default is NULL.\n#' \n#' @details Use either `renvlock_path` or `dependencies` to install required\n#' packages, not both. By default, both are `NULL`, so the Docker image will\n#' have no additional packages installed.\nTo include any support scripts, provide a character vector script paths to the support_path argument when building the Lambda docker image locally with build_lamdba.\n[Note that, multi-file project was supported previously as well, although perhaps not explicitly. An approach that I like is to create an ‘R’ package that exports the runtime function needed for the Lambda. Then one just needs to make that custom R package a dependency of the project and either install in the AWS Lambda docker image it through dependencies or renvlock_path.]\nTo use an existing renv.lock for installation of dependencies, provide its path to the renvlock_path argument to build_lambda. This instructs the code to copy the renv.lock file to the image and run renv::restore() which will reconstruct the R environment inside the docker image. I really like this feature, as it minimizes the size of the Dockefile and removes some potential headaches with R package dependencies from different repositories (CRAN, BioConductor, GitHub, etc)."
  },
  {
    "objectID": "posts/r2lambda/2024-10-17-r2lambda-renv/index.html#deploy-a-project-with-multiple-r-scripts-and-renv-managed-environment-to-aws-lambda",
    "href": "posts/r2lambda/2024-10-17-r2lambda-renv/index.html#deploy-a-project-with-multiple-r-scripts-and-renv-managed-environment-to-aws-lambda",
    "title": "r2lambda update to support multi-file projects and renv projects",
    "section": "",
    "text": "It has been a while since I’ve had the chance to work on my {r2lambda} project. In particular, there were a couple of good points made by a user on GitHub about functionality that is missing from the package. The option to deploy multiple files, e.g., one runtime function that depends on helpers in the same project organized in different files. And another, to enable {renv} management of the R environment within the AWS Lambda docker image. Both excellent points that I wished I addressed earlier. But better late than never.\nBoth of these features required minor adjustments to the codebase. Copying additional supports scripts and restoring the {renv} environment should both happen when the AWS Lambda docker image is built, so the logic to create the Dockerfile needed to be updated. Accordinly, the r2lambda::build_lambda function now has two additional arguments:\n#' @param support_path path to the support files (if any). Either NULL \n#' (the default) if all needed code is in the same `runtime_path` script, or a \n#' character vector of paths to additional files needed by the runtime script.\n#' @param renvlock_path path to the renv.lock file (if any). Default is NULL.\n#' \n#' @details Use either `renvlock_path` or `dependencies` to install required\n#' packages, not both. By default, both are `NULL`, so the Docker image will\n#' have no additional packages installed.\nTo include any support scripts, provide a character vector script paths to the support_path argument when building the Lambda docker image locally with build_lamdba.\n[Note that, multi-file project was supported previously as well, although perhaps not explicitly. An approach that I like is to create an ‘R’ package that exports the runtime function needed for the Lambda. Then one just needs to make that custom R package a dependency of the project and either install in the AWS Lambda docker image it through dependencies or renvlock_path.]\nTo use an existing renv.lock for installation of dependencies, provide its path to the renvlock_path argument to build_lambda. This instructs the code to copy the renv.lock file to the image and run renv::restore() which will reconstruct the R environment inside the docker image. I really like this feature, as it minimizes the size of the Dockefile and removes some potential headaches with R package dependencies from different repositories (CRAN, BioConductor, GitHub, etc)."
  },
  {
    "objectID": "posts/r2lambda/2024-10-17-r2lambda-renv/index.html#demo-code",
    "href": "posts/r2lambda/2024-10-17-r2lambda-renv/index.html#demo-code",
    "title": "r2lambda update to support multi-file projects and renv projects",
    "section": "Demo code",
    "text": "Demo code\nAssuming we have a folder with the following structure:\n~/Desktop$ ls -1 iris-lambda/\nrenv/\nrenv.lock\nruntime.r\nsupport.r\ntest-code.r\nWhere, support.r defines some function that runtime.r uses for the Lambda:\nget_iris_summary_by_species &lt;- function(species) {\n    iris |&gt;\n    dplyr::filter(Species == species) |&gt;\n    dplyr::summarise(\n      mean = mean(Sepal.Length),\n      sd = sd(Sepal.Length)\n    )\n}\nThen runtime.r, sources the support script, and calls the function defined there:\nsource(\"support.r\")\n\niris_summary &lt;- function(species) {\n  get_iris_summary_by_species(species)\n}\n\nlambdr::start_lambda()\nThen the following should work, passing the support script and renv.lock to r2lambda::build_lambda:\ndir(\"~/Desktop/iris-lambda\")\nruntime_function &lt;- \"iris_summary\"\nruntime_path &lt;- \"~/Desktop/iris-lambda/runtime.r\"\nsupport_path &lt;- \"~/Desktop/iris-lambda/support.r\"\nrenvlock_path &lt;- \"~/Desktop/iris-lambda/renv.lock\"\ndependencies &lt;- NULL\n\n# Might take a while, its building a docker image\nbuild_lambda(\n  tag = \"my_iris_lambda\",\n  runtime_function = runtime_function,\n  runtime_path = runtime_path,\n  support_path = support_path,\n  renvlock_path = renvlock_path,\n  dependencies = dependencies\n)\n\n# test\npayload &lt;- list(species = \"setosa\")\ntag &lt;- \"my_iris_lambda\"\ntest_lambda(tag = tag, payload)\n\n\n# deploy\n\n# Might take a while, its pushing it to a remote repository\ndeploy_lambda(\n  tag = \"my_iris_lambda\",\n  Timeout = 30\n)\n\ninvoke_lambda(\n  function_name = \"my_iris_lambda\",\n  invocation_type = \"RequestResponse\",\n  payload = list(species = \"versicolor\"),\n  include_logs = FALSE\n)\n\ninvoke_lambda(\n  function_name = \"my_iris_lambda\",\n  invocation_type = \"RequestResponse\",\n  payload = list(species = \"setosa\"),\n  include_logs = FALSE\n)"
  },
  {
    "objectID": "posts/r2lambda/2023-02-04-deploy-an-r-script-as-an-aws-lambda-function-without-leaving-the-r-console/index.html",
    "href": "posts/r2lambda/2023-02-04-deploy-an-r-script-as-an-aws-lambda-function-without-leaving-the-r-console/index.html",
    "title": "Deploy an R script as an AWS Lambda function without leaving the R console",
    "section": "",
    "text": "AWS Lambda is a serverless compute service that lets us deploy any code as a cloud function without worrying about setting up a compute server (for example like discussed here and here). The code can be deployed as a zip archive or a container image. In the R case, only the container option is available and enabled by the {lambdr} package (see also this workflow). The deployment using {lambdr}procedure involves:\n\nwriting an R script to serve as the runtime function\n\ncreating a Lambda Dockerfile and docker image locally\n\npushing the docker image to AWS Elastic Container Registry (ECR)\n\ncreating a Lambda function using the ECR image either with the AWS command-line interface (cli) or in the web console\n\nGoing through this procedure is a rewarding experience, as it teaches many things related to docker containers, AWS setup, using the AWS cli, etc. But, once you have gone through the this procedure a few times, it becomes a bit cumbersome and time-consuming to navigate between an R console, the shell, and aws cli or the AWS web console in a browser to write and update the code, re-create the docker image, push, and then re-create the Lambda function. It would be nice to have a “one-call” solution, where we call a single function, point it to a file that contains the code we wish to deploy, and sit back while our R session goes through the steps.\nIn this post, I will introduce an R package, called {r2lambda}, that aims to make it easier to deploy R code as AWS Lambda function by automating the above procedure."
  },
  {
    "objectID": "posts/r2lambda/2023-02-04-deploy-an-r-script-as-an-aws-lambda-function-without-leaving-the-r-console/index.html#system-dependencies",
    "href": "posts/r2lambda/2023-02-04-deploy-an-r-script-as-an-aws-lambda-function-without-leaving-the-r-console/index.html#system-dependencies",
    "title": "Deploy an R script as an AWS Lambda function without leaving the R console",
    "section": "System dependencies",
    "text": "System dependencies\nThe only system dependency is docker, because R lambdas are docker images, so we are not going anywhere unless docker is installed."
  },
  {
    "objectID": "posts/r2lambda/2023-02-04-deploy-an-r-script-as-an-aws-lambda-function-without-leaving-the-r-console/index.html#r-dependencies",
    "href": "posts/r2lambda/2023-02-04-deploy-an-r-script-as-an-aws-lambda-function-without-leaving-the-r-console/index.html#r-dependencies",
    "title": "Deploy an R script as an AWS Lambda function without leaving the R console",
    "section": "R dependencies",
    "text": "R dependencies\nThe core R dependencies are {lambdr}, {stevedore}, and {paws}.\n{lambdr} provides the R runtime for AWS Lambda. In practice, the most important points about using {lambdr} are 1) to setup the Dockerfile:\n\ninstallation of system dependencies\n\ninstallation of R dependencies of the runtime function\n\nsetting the runtime function to be run by the container via CMD,\n\nand 2) to setup the R script by adding lambdr::start_lambda() at the bottom.\n{stevedore} is a docker client for R. It provides an interface to the docker API. In the context of deploiyng Lambda functions, it is used to list and tag local images, and to login and push images to the AWS ECR repository. Using {stevedore} simplifies how {r2lambda} works in two ways. First, we don’t need to use system calls to run {docker} commands, and second, we don’t depend on the aws cli.\n{paws} is the R software development kit for AWS. {r2lambda} uses {paws} to connect to the AWS services using your credentials, to create execution roles for the Lambda, to create the Lambda it self, and to invoke the Lambda function.\nThe remaining R dependencies provide features for input validation and testing ({checkmate}), logging ({logger}), and text interpolation ({glue})."
  },
  {
    "objectID": "posts/r2lambda/2023-02-04-deploy-an-r-script-as-an-aws-lambda-function-without-leaving-the-r-console/index.html#environmental-variables-for-aws-configuration",
    "href": "posts/r2lambda/2023-02-04-deploy-an-r-script-as-an-aws-lambda-function-without-leaving-the-r-console/index.html#environmental-variables-for-aws-configuration",
    "title": "Deploy an R script as an AWS Lambda function without leaving the R console",
    "section": "Environmental variables for AWS configuration",
    "text": "Environmental variables for AWS configuration\nOur AWS credentials are needed so that functions that use the paws SDK can authenticate with AWS. This is a simple .Renviron:\nACCESS_KEY_ID = \"YOUR AWS ACCESS KEY ID\"\nSECRET_ACCESS_KEY = \"YOUR AWS SECRET ACCESS KEY\"\nPROFILE = \"YOUR AWS PROFILE\"\nREGION = \"YOUR AWS REGION\""
  },
  {
    "objectID": "posts/r2lambda/2023-02-04-deploy-an-r-script-as-an-aws-lambda-function-without-leaving-the-r-console/index.html#installation",
    "href": "posts/r2lambda/2023-02-04-deploy-an-r-script-as-an-aws-lambda-function-without-leaving-the-r-console/index.html#installation",
    "title": "Deploy an R script as an AWS Lambda function without leaving the R console",
    "section": "Installation",
    "text": "Installation\nOnce the prerequisites are ready, we can install {r2lambda} from GitHub using {remotes}:\n# install_packages(\"remotes\")\nremotes::install_github(\"discindo\", \"r2lambda\")"
  },
  {
    "objectID": "posts/r2lambda/2023-02-04-deploy-an-r-script-as-an-aws-lambda-function-without-leaving-the-r-console/index.html#demo-run-with-logs",
    "href": "posts/r2lambda/2023-02-04-deploy-an-r-script-as-an-aws-lambda-function-without-leaving-the-r-console/index.html#demo-run-with-logs",
    "title": "Deploy an R script as an AWS Lambda function without leaving the R console",
    "section": "Demo run with logs",
    "text": "Demo run with logs\nA full deployment and invocation run with a demo runtime script should look like the code and output below.\n&gt;   runtime_function &lt;- \"parity\"\n&gt;   runtime_path &lt;- system.file(\"parity.R\", package = \"r2lambda\")\n&gt;   dependencies &lt;- NULL\n&gt; \n&gt;   deploy_lambda(\n+     tag = \"parity-test1\",\n+     runtime_function = runtime_function,\n+     runtime_path = runtime_path,\n+     dependencies = dependencies\n+     )\nINFO [2023-01-29 20:32:41] [deploy_lambda] Checking system dependencies (`aws cli`, `docker`).\n/usr/bin/docker\nINFO [2023-01-29 20:32:41] [deploy_lambda] Creating temporary working directory.\nINFO [2023-01-29 20:32:41] [deploy_lambda] Creating Dockerfile.\nWARN [2023-01-29 20:32:41] [deploy_lambda] Created Dockerfile and lambda runtime script in temporary folder.\nINFO [2023-01-29 20:32:41] [deploy_lambda] Building Docker image.\nSending build context to Docker daemon  3.584kB\nStep 1/13 : FROM public.ecr.aws/lambda/provided\n ---&gt; ccae8d728af2\nStep 2/13 : ENV R_VERSION=4.0.3\n ---&gt; Using cache\n ---&gt; bf3dd3c804f3\nStep 3/13 : RUN yum -y install wget git tar\n ---&gt; Using cache\n ---&gt; 8b82b80771cf\nStep 4/13 : RUN yum -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm   && wget https://cdn.rstudio.com/r/centos-7/pkgs/R-${R_VERSION}-1-1.x86_64.rpm   && yum -y install R-${R_VERSION}-1-1.x86_64.rpm   && rm R-${R_VERSION}-1-1.x86_64.rpm\n ---&gt; Using cache\n ---&gt; c98bc560eff4\nStep 5/13 : ENV PATH=\"${PATH}:/opt/R/${R_VERSION}/bin/\"\n ---&gt; Using cache\n ---&gt; 4565b8100c39\nStep 6/13 : RUN yum -y install openssl-devel\n ---&gt; Using cache\n ---&gt; d8e46fe52a6d\nStep 7/13 : RUN Rscript -e \"install.packages(c('httr', 'jsonlite', 'logger', 'remotes'), repos = 'https://packagemanager.rstudio.com/all/__linux__/centos7/latest')\"\n ---&gt; Using cache\n ---&gt; 46ca4b6e95a0\nStep 8/13 : RUN Rscript -e \"remotes::install_github('mdneuzerling/lambdr')\"\n ---&gt; Using cache\n ---&gt; 67283a940985\nStep 9/13 : RUN mkdir /lambda\n ---&gt; Using cache\n ---&gt; d6762390f9a9\nStep 10/13 : COPY runtime.R /lambda\n ---&gt; Using cache\n ---&gt; 94af1e345ecc\nStep 11/13 : RUN chmod 755 -R /lambda\n ---&gt; Using cache\n ---&gt; cd15870ad843\nStep 12/13 : RUN printf '#!/bin/sh\\ncd /lambda\\nRscript runtime.R' &gt; /var/runtime/bootstrap   && chmod +x /var/runtime/bootstrap\n ---&gt; Using cache\n ---&gt; 66d74d4de62e\nStep 13/13 : CMD [\"parity\"]\n ---&gt; Using cache\n ---&gt; e47d4fea17a1\nSuccessfully built e47d4fea17a1\nSuccessfully tagged parity-test1:latest\nWARN [2023-01-29 20:32:41] [deploy_lambda] Docker image built. This can take up substantial amount of disk space.\nWARN [2023-01-29 20:32:41] [deploy_lambda] Use `docker image ls` in your shell to see the image size.\nWARN [2023-01-29 20:32:41] [deploy_lambda] Use `docker rmi &lt;image&gt;` in your shell to remove an image.\nINFO [2023-01-29 20:32:41] [deploy_lambda] Pushing Docker image to AWS ECR.\n\n... [truncated]\n\nLogin Succeeded\nThe push refers to repository [*.dkr.ecr.us-east-1.amazonaws.com/parity-test1]\n\n... [truncated]\n\nlatest: digest: sha256:9f38150cf89bf6a3f7d95c853105afe82616f85f3afbb65d7f71d2f1400dedeb size: 3045\nWARN [2023-01-29 20:45:25] [deploy_lambda] Docker image pushed to ECR. This can take up substantial resources and incur cost.\nWARN [2023-01-29 20:45:25] [deploy_lambda] Use `paws::ecr()`, the AWS CLI, or the AWS console to manage your images.\nINFO [2023-01-29 20:45:25] [deploy_lambda] Creating Lambda role and basic policy.\nWARN [2023-01-29 20:45:26] [deploy_lambda] Created AWS role with basic lambda execution permissions.\nWARN [2023-01-29 20:45:26] [deploy_lambda] Use `paws::iam()`, the AWS CLI, or the AWS console to manage your roles, and permissions.\nINFO [2023-01-29 20:45:36] [deploy_lambda] Creating Lambda function from image.\nWARN [2023-01-29 20:45:37] [deploy_lambda] Lambda function created. This can take up substantial resources and incur cost.\nWARN [2023-01-29 20:45:37] [deploy_lambda] Use `paws::lambda()`, the AWS CLI, or the AWS console to manage your functions.\nWARN [2023-01-29 20:45:37] [deploy_lambda] Lambda function created successfully.\nWARN [2023-01-29 20:45:37] [deploy_lambda] Pushed docker image to ECR with URI `*.dkr.ecr.us-east-1.amazonaws.com/parity-test1`\nWARN [2023-01-29 20:45:37] [deploy_lambda] Created Lambda execution role with ARN `arn:aws:iam::*:role/parity-test1--261b7f62-a048-11ed-bd89-10c37b6dce99`\nWARN [2023-01-29 20:45:37] [deploy_lambda] Created Lambda function `parity-test1` with ARN `arn:aws:lambda:us-east-1:*:function:parity-test1`\nSUCCESS [2023-01-29 20:45:37] [deploy_lambda] Done.\n&gt; \n&gt;  invoke_lambda(\n+    function_name = \"parity-test1\",\n+    payload = list(number = 3),\n+    invocation_type = \"RequestResponse\"\n+   )\nINFO [2023-01-29 20:45:37] [invoke_lambda] Validating inputs.\nINFO [2023-01-29 20:45:37] [invoke_lambda] Invoking function.\nError: ResourceConflictException (HTTP 409). The operation cannot be performed at this time. The function is currently in the following state: Pending\n\n&gt;  invoke_lambda(\n+    function_name = \"parity-test1\",\n+    payload = list(number = 3),\n+    invocation_type = \"RequestResponse\"\n+   )\nINFO [2023-01-29 21:32:53] [invoke_lambda] Validating inputs.\nINFO [2023-01-29 21:32:53] [invoke_lambda] Invoking function.\n\nLambda response payload: \n{\"parity\":\"odd\"}\nSUCCESS [2023-01-29 21:33:02] [invoke_lambda] Done.\nThe code is purposely verbose to let the user know what actions are being taken and what resources are being set up on AWS. Of course, setting up and running services on AWS incurs costs, so it is always a good idea to review your AWS console and disable or remove services that are no longer needed. Actions like these can also be done with the paws SDK, and I hope that a future iteration of {r2lamdba} might make some AWS clean up possible from the R console. For now, deploy_lambda will specify the URI or ARN of each service it creates both as a console log, and in the returned list. So the user can easily find the created services and disable/remove/update as needed. As of now, {r2lambda} interacts only with IAM, ECR, and Lambda, so be sure at least to log into AWS, and review the actions taken."
  },
  {
    "objectID": "r2lambda-series.html",
    "href": "r2lambda-series.html",
    "title": "Series: r2lambda",
    "section": "",
    "text": "An R AWS Lambda function to download Tidytuesday datasets\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\nteo\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy an R script as an AWS Lambda function without leaving the R console\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2023\n\n\nTeo\n\n\n\n\n\n\n\n\n\n\n\n\nHow to invoke an AWS Lambda function with R and paws\n\n\n\n\n\n\n\n\n\n\n\nJul 24, 2022\n\n\nTeo\n\n\n\n\n\n\n\n\n\n\n\n\nHow to set up an R-based AWS Lambda to write to AWS S3 on a schedule\n\n\nUse AWS Lambda to save the Tidytuesday dataset to AWS S3 every Wednesday\n\n\n\n\n\n\n\n\nMar 9, 2023\n\n\nteo\n\n\n\n\n\n\n\n\n\n\n\n\nTest an R-based AWS lambda function locally before deploying to the cloud\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2023\n\n\nteo\n\n\n\n\n\n\n\n\n\n\n\n\nr2lambda update to support multi-file projects and renv projects\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2024\n\n\nTeo\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#data-are-fun",
    "href": "index.html#data-are-fun",
    "title": "Discindo",
    "section": "",
    "text": "Data science and data engineering solutions with R and Shiny in the Cloud"
  },
  {
    "objectID": "series.html",
    "href": "series.html",
    "title": "Series",
    "section": "",
    "text": "This series contains…."
  },
  {
    "objectID": "novica/index.html",
    "href": "novica/index.html",
    "title": "Novica Nakov",
    "section": "",
    "text": "👋Hey,\nI currently work with data and people at the 🐟 Norwegian 🐄 Veterinary 🥘 Institute.\n\n💻I write and contribute to R packages;\n©️ I like to talk about free software;\n✍️ I am a co-author at the Discindo blog which mostly covers topics about R and Shiny in the cloud;\n📚 I write books, most recently for children;\n👨‍🎓 I like to learn new things;\n🦎 I used to contribute a lot to l10n of free software projects, Mozilla in particular. My name is on the Mozilla monument;\n📄 I keep my CV in plain text on this GitHub repository, or as HTML on GitHub Pages;\n🍕and, the name Novica rhymes with pizza."
  },
  {
    "objectID": "series.html#here-goes",
    "href": "series.html#here-goes",
    "title": "Series",
    "section": "",
    "text": "This series contains…."
  },
  {
    "objectID": "authors/novica/index.html",
    "href": "authors/novica/index.html",
    "title": "Novica Nakov",
    "section": "",
    "text": "👋Hey,\nI currently work with data and people at the 🐟 Norwegian 🐄 Veterinary 🥘 Institute.\n\n💻I write and contribute to R packages;\n©️ I like to talk about free software;\n✍️ I am a co-author at the Discindo blog which mostly covers topics about R and Shiny in the cloud;\n📚 I write books, most recently for children;\n👨‍🎓 I like to learn new things;\n🦎 I used to contribute a lot to l10n of free software projects, Mozilla in particular. My name is on the Mozilla monument;\n📄 I keep my CV in plain text on this GitHub repository, or as HTML on GitHub Pages;\n🍕and, the name Novica rhymes with pizza."
  },
  {
    "objectID": "projects/newscatcheR/index.html",
    "href": "projects/newscatcheR/index.html",
    "title": "newscatcheR",
    "section": "",
    "text": "Programmatically collect normalized news from (almost) any website using R.\nnewscatcheR is an R clone of the python package newscatcher.\nThe package provides a dataset of news sites and their rss feeds, together with some characteristics of the websites such as the topic, country or language of the website, and few functions explore and access the feeds from R."
  },
  {
    "objectID": "projects/r2lambda/index.html",
    "href": "projects/r2lambda/index.html",
    "title": "r2lambda",
    "section": "",
    "text": "The goal of {r2lambda} is to make it easier to go from an R script to a deployed AWS Lambda function. It does this by wrapping preexisting functionality from the {paws} and {lambdr} packages."
  }
]