{
  "hash": "ec3dc18adb2edca505d230e13d8a47fa",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'How to set up an R-based AWS Lambda to write to AWS S3 on a schedule'\nauthor: 'teo'\ndate: '2023-03-09'\nexecute:\n  eval: false\nslug: lambda-to-s3\ncategories:\n  - AWS\n  - R\ntags:\n  - r2lambda\n  - AWS Lambda\n  - AWS EventBridge\n  - AWS S3\n  - cron\nsubtitle: 'Use AWS Lambda to save the Tidytuesday dataset to AWS S3 every Wednesday'\nsummary: 'An introduction to working with AWS S3 from R and a step-by-step workflow to set an AWS Lambda functon to save datasets to an S3 bucket on a weekly schedule'\nauthors: [teo]\nlastmod: '2023-03-09T10:22:06-06:00'\nfeatured: no\nprojects: ['r2lambda']\nshow_related: true\n---\n\n\n\n\n\n\n## Overview\n\nAt the end of this tutorial, we would have created an AWS Lambda function that fetches\nthe most-recent Tidytuesday dataset and writes it into an S3 Bucket every Wednesday.\nTo do this, we'll first work interactively with `{r2lambda}` and `{paws}` to go \nthrough all the steps the Lambda function would eventually need to do, then wrap\nthe code and deploy it to AWS Lambda, and finally schedule it to run weekly. \n\n## Getting started with AWS Simple Storage Service (S3) from R\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(r2lambda)\nlibrary(tidytuesdayR)\n```\n:::\n\n\n\n\n\nAs with any AWS service supported by `{paws}`, we can easily connect to S3 and \nperform some basic operations. Below, we establish an S3 service using `r2lambda::aws_connect`,\nthen create a bucket called `tidytuesday-dataset`, drop and then delete and empty file, and \ndelete the bucket altogether. This exercise is not very meaningful beyond learning\nthe basics on how to interact with S3 from `R`. Eventually, though, our lambda function\nwould need to do something similar, so being familiar with the process in an interactive\nsession helps.\n\n**To run any of the code below, you need some environmental variables set. See \nthe [Setup](https://github.com/discindo/r2lambda#build-a-docker-image-for-the-lambda-function) \nsection in the `{r2lambda}` package readme for more details**\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns3_service <- aws_connect(\"s3\")\n\n# create a bucket on S3\ns3_service$create_bucket(Bucket = \"a-unique-bucket\")\n\n# upload an object to our bucket\ntmpfile <- tempfile(pattern = \"object_\", fileext = \"txt\")\nwrite(\"test\", tmpfile)\n(readLines(tmpfile))\ns3_service$put_object(Body = tmpfile, Bucket = \"a-unique-bucket\", Key = \"TestFile\")\n\n# list the contents of a bucket\ns3_service$list_objects(Bucket = \"a-unique-bucket\")\n\n# delete an object from a bucket\ns3_service$delete_object(Bucket = \"a-unique-bucket\", Key = \"TestFile\")\n\n# delete a bucket\ns3_service$delete_bucket(Bucket = \"a-unique-bucket\")\n```\n:::\n\n\n\n\n\nNow, the above procedure used a local file, but what if we generated some data during\nour session, and we want to stream that directly to S3 without saving to file? In\nmany cases, we don't have the option to write to disk or simply don't want to.\n\nIn such cases we need to serialize our data object before trying to `put` it in the bucket.\nThis comes down to calling `serialize` with `connection=NULL` to generate a `raw` \nvector without writing to a file. We can then put the `iris` data set from memory into our \n`a-unique-bucket` S3 bucket.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns3_service <- aws_connect(\"s3\")\n\n# create a bucket on S3\ns3_service$create_bucket(Bucket = \"a-unique-bucket\")\n\n# upload an object to our bucket\nsiris <- serialize(iris, connection = NULL)\ns3_service$put_object(Body = siris, Bucket = \"a-unique-bucket\", Key = \"TestFile2\")\n\n# list the contents of a bucket\ns3_service$list_objects(Bucket = \"a-unique-bucket\")\n\n# delete an object from a bucket\ns3_service$delete_object(Bucket = \"a-unique-bucket\", Key = \"TestFile2\")\n\n# delete a bucket\ns3_service$delete_bucket(Bucket = \"a-unique-bucket\")\n```\n:::\n\n\n\n\n\nOK. With that, we now know the two steps our Lambda function would need to do:\n\n  1. fetch the most recent Tidytuesday data set (see \n  [this post](https://discindo.org/post/an-r-aws-lambda-function-to-download-tidytuesday-datasets/) for details)\n  2. put the data set as an object in the S3 bucket\n  \nStill in an interactive session, lets just write the code that our Lambda would have\nto execute.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidytuesdayR)\n\n# Find the most recent tuesday and fetch the corresponding data set\nmost_recent_tuesday <- tidytuesdayR::last_tuesday(date = Sys.Date())\ntt_data <- tidytuesdayR::tt_load(x = most_recent_tuesday)\n\n# by default it comes as class `tt_data`, which causes problems\n# with serialization and conversion to JSON. So best to extract\n# the data set(s) as a simple list\ntt_data <- lapply(names(tt_data), function(x) tt_data[[x]])\n\n# then serialize\ntt_data_raw <- serialize(tt_data, connection = NULL)\n\n# create a bucket on S3\ns3_service <- r2lambda::aws_connect(\"s3\")\ns3_service$create_bucket(Bucket = \"tidytuesday-datasets\")\n\n# upload an object to our bucket\ns3_service$put_object(\n  Body = tt_data_raw, \n  Bucket = \"tidytuesday-datasets\", \n  Key = most_recent_tuesday\n)\n\n# list the contents of our bucket and find the Keys for all objects\nobjects <- s3_service$list_objects(Bucket = \"tidytuesday-datasets\")\nsapply(objects$Contents, \"[[\", \"Key\")\n#> [1] \"2023-03-07\"\n\n# fetch a Tidytuesday dataset from S3\ntt_dataset <- s3_service$get_object(\n  Bucket = \"tidytuesday-datasets\", \n  Key = most_recent_tuesday\n)\n\n# convert from raw and show the first few rows\ntt_dataset$Body %>% unserialize() %>% head()\n```\n:::\n\n\n\n\n\nNow we should have everything we need to write our Lambda function.\n\n## Lambda + S3 integration: Dropping a file in an S3 bucket\n\nWrapping the above interactive code into a function and also, defining an `s3_connect`\nfunction as a helper to create an S3 client within the function. By doing this, we\navoid adding `r2lambda` as a dependency to the Lambda function. (At the time of writing,\n`r2lambda` does not yet support non-CRAN packages.)\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns3_connect <- function() {\n  paws::s3(config = list(\n    credentials = list(\n      creds = list(\n        access_key_id = Sys.getenv(\"ACCESS_KEY_ID\"),\n        secret_access_key = Sys.getenv(\"SECRET_ACCESS_KEY\")\n      ),\n      profile = Sys.getenv(\"PROFILE\")\n    ),\n    region = Sys.getenv(\"REGION\")\n  ))\n}\n\ntidytuesday_lambda_s3 <- function() {\n  most_recent_tuesday <- tidytuesdayR::last_tuesday(date = Sys.Date())\n  tt_data <- tidytuesdayR::tt_load(x = most_recent_tuesday)\n  tt_data <- lapply(names(tt_data), function(x) tt_data[[x]])\n  tt_data_raw <- serialize(tt_data, connection = NULL)\n  \n  s3_service <- s3_connect()\n  s3_service$put_object(Body = tt_data_raw,\n                        Bucket = \"tidytuesday-datasets\",\n                        Key = most_recent_tuesday)\n  \n}\n```\n:::\n\n\n\n\n\nNow, calling `tidytuesday_lambda_s3()` should fetch and put the most recent \nTidytuesday data set into our S3 bucket. To test it, we run:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidytuesday_lambda_s3()\n\nlist_objects <- function(bucket) {\n  s3 <- s3_connect()\n  obj <- s3$list_objects(Bucket = bucket)\n  sapply(obj$Contents, \"[[\", \"Key\")\n}\n\nlist_objects(\"tidytuesday-datasets\")\n#> [1] \"2023-03-07\"\n```\n:::\n\n\n\n\n\nOn to the next step, to create and deploy the Lambda function. We have a few \nconsiderations here:\n\n1. For the Lambda function to connect to S3, it needs access to some environmental \nvariables. The same ones as we have in our current interactive session without which\nwe can't establish local clients of AWS services. These are: `REGION`, `PROFILE`,\n`SECRET_ACCESS_KEY`, and `ACCESS_KEY_ID`. To include these envvars in the Lambda\ndocker image on deploy, use the `set_aws_envvars` argument of `deploy_lambda`.\n\n2. We have some dependencies that would need to be available in the docker image. \nWe already saw how to install `{tidytuesdayR}` in our Lambda docker image in a \n[previous post](https://discindo.org/post/an-r-aws-lambda-function-to-download-tidytuesday-datasets/).\nBesides this, we also need to install `{paws}`, because without it we can't interact\nwith S3. To do this, we just need to add `dependencies = c(\"tidytuesdayR\", \"paws\")` \nwhen building the image with `r2lambda::build_lambda`.\n\n### Build\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr_code <- \"\n  s3_connect <- function() {\n    paws::s3(config = list(\n      credentials = list(\n        creds = list(\n          access_key_id = Sys.getenv('ACCESS_KEY_ID'),\n          secret_access_key = Sys.getenv('SECRET_ACCESS_KEY')\n        ),\n        profile = Sys.getenv('PROFILE')\n      ),\n      region = Sys.getenv('REGION')\n    ))\n  }\n  \n  tidytuesday_lambda_s3 <- function() {\n    most_recent_tuesday <- tidytuesdayR::last_tuesday(date = Sys.Date())\n    tt_data <- tidytuesdayR::tt_load(x = most_recent_tuesday)\n    tt_data <- lapply(names(tt_data), function(x) tt_data[[x]])\n    tt_data_raw <- serialize(tt_data, connection = NULL)\n    \n    s3_service <- s3_connect()\n    s3_service$put_object(Body = tt_data_raw,\n                          Bucket = 'tidytuesday-datasets',\n                          Key = most_recent_tuesday)\n  }\n  \n  lambdr::start_lambda()\n\"\n\ntmpfile <- tempfile(pattern = \"tt_lambda_s3_\", fileext = \".R\")\nwrite(x = r_code, file = tmpfile)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nruntime_function <- \"tidytuesday_lambda_s3\"\nruntime_path <- tmpfile\ndependencies <- c(\"tidytuesdayR\", \"paws\")\n\nr2lambda::build_lambda(\n  tag = \"tidytuesday_lambda_s3\",\n  runtime_function = runtime_function,\n  runtime_path = runtime_path,\n  dependencies = dependencies\n)\n```\n:::\n\n\n\n\n\n### Deploy\n\nWe set a generous 2 minute timeout, just to be safe that the data set is successfully\ncopied to S3. And we also increase the available memory to 1024 mb. Note also the \nflag to pass along our local AWS envvars to the deployed lambda environment.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr2lambda::deploy_lambda(\n  tag = \"tidytuesday_lambda_s3\",\n  set_aws_envvars = TRUE,\n  Timeout = 120,\n  MemorySize = 1024)\n```\n:::\n\n\n\n\n\n### Invoke\n\nWe invoke as usual, with an empty list as payload because our function does not \ntake any arguments.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr2lambda::invoke_lambda(\n  function_name = \"tidytuesday_lambda_s3\", \n  invocation_type = \"RequestResponse\", \n  payload = list(),\n  include_logs = TRUE)\n\n#> INFO [2023-03-08 23:50:46] [invoke_lambda] Validating inputs.\n#> INFO [2023-03-08 23:50:46] [invoke_lambda] Checking function state.\n#> INFO [2023-03-08 23:50:47] [invoke_lambda] Function state: Active.\n#> INFO [2023-03-08 23:50:47] [invoke_lambda] Invoking function.\n#> \n#> Lambda response payload: \n#> {\"Expiration\":[],\"ETag\":\"\\\"4f5a6085215b9074faed28d816696a99\\\"\",\"ChecksumCRC32\":[],\n#> \"ChecksumCRC32C\":[],\"ChecksumSHA1\":[],\"ChecksumSHA256\":[],\"ServerSideEncryption\":\"AES256\",\n#> \"VersionId\":[],\"SSECustomerAlgorithm\":[],\"SSECustomerKeyMD5\":[],\"SSEKMSKeyId\":[],\n#> \"SSEKMSEncryptionContext\":[],\"BucketKeyEnabled\":[],\"RequestCharged\":[]}\n#> \n#> Lambda logs: \n#> OpenBLAS WARNING - could not determine the L2 cache size on this system, assuming 256k\n#> INFO [2023-03-09 05:50:49] Using handler function  tidytuesday_lambda_s3\n#> START RequestId: c6cb0600-3400-4ca3-9232-8af53542f8e8 Version: $LATEST\n#> --- Compiling #TidyTuesday Information for 2023-03-07 ----\n#> --- There is 1 file available ---\n#> --- Starting Download ---\n#> Downloading file 1 of 1: `numbats.csv`\n#> --- Download complete ---\n#> END RequestId: c6cb0600-3400-4ca3-9232-8af53542f8e8\n#> REPORT RequestId: c6cb0600-3400-4ca3-9232-8af53542f8e8\tDuration: 12061.06 ms\t\n#> Billed Duration: 13331 ms\tMemory Size: 1024 MB\tMax Memory Used: 181 MB\tInit \n#> Duration: 1269.59 ms\t\n#> SUCCESS [2023-03-08 23:51:01] [invoke_lambda] Done.\n```\n:::\n\n\n\n\n\nThen, to confirm that a Tidytuesday data set was written to S3 as an object in the \nbucket `tidytuesday-datasets` we would run:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns3_service <- r2lambda::aws_connect(service = \"s3\")\nobjs <- s3_service$list_objects(Bucket = \"tidytuesday-datasets\")\nobjs$Contents[[1]]$Key\n#> [1] \"2023-03-07\"\n```\n:::\n\n\n\n\n\nWe expect to see one object with a `Key` matching the date of the most recent Tuesday.\nAt the time of writing that is March 7, 2023.\n\n### Schedule\n\nFinally, to copy the Tidytuesday dataset on a weekly basis, for example, every Wednesday,\nwe would use `r2lambda::schedule_lambda` with an execution rate set by `cron`.\n\nFirst, to validate that things are working, we can set the lambda on a 5-minute\nschedule and check the time stamp on the on the S3 object to make sure it is updated\nevery 5 minutes:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# schedule the lambda to execute every 5 minutes\nr2lambda::schedule_lambda(\n  lambda_function = \"tidytuesday_lambda_s3\", \n  execution_rate = \"rate(5 minutes)\"\n  )\n\n# occasionally query the S3 bucket status and the LastModified time stamp\nobjs <- s3_service$list_objects(Bucket = \"tidytuesday-datasets\")\nobjs$Contents[[1]]$LastModified\n```\n:::\n\n\n\n\n\nIf all is well, set it to run every Wednesday at midnight:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr2lambda::schedule_lambda(\n  lambda_function = \"tidytuesday_lambda_s3\",\n  execution_rate = \"cron(0 0 * * Wed *)\"\n  )\n```\n:::\n\n\n\n\n\nNext Wednesday morning, we should have two objects, with keys matching the two \nmost-recent Tuesdays.\n\n## Summary\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}