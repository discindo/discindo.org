{
  "hash": "aefe644a7f4a53d2f44ab2772dea5e4f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Using R and {paws} to populate DynamoDB tables #2'\nauthors: ['teo']\ndate: '2022-05-18'\nslug: []\ncategories:\n  - r\ntags:\n  - paws\n  - aws\n  - dynamodb\nimages: []\n---\n\n\n\n\n\nIn our [previous post](https://www.discindo.org/post/2022-04-30-using-r-and-paws-to-populate-dynamodb-tables/) we covered some basics of using the `paws` `SDK` to interact with AWS `DynamoDB` from `R`. We wrote a few simple functions to prepare `R` lists in the appropriate format for `DynamoDB` and some wrappers to put these lists as items in the remote NoSQL database. \n\nThese are good first steps, but we can't really use these functions in a production setting, where we might need to populate a table with a few thousand or few million rows. In this post we'll extend our work to accommodate these\nmore realistic needs.\n\n## Creating `DynamoDB` tables from `R`\n\nWe first create a function to wrap the call to `paws::dynamodb()` that we use to establish a client for the cloud database service. We assume the credentials are stored in an `.Renviron` file and available in `R` as environmental variables through `Sys.getenv`. Our connection function then is simply:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndynamo_connect <- function() {\n  paws::dynamodb(config = list(\n    credentials = list(\n      creds = list(\n        access_key_id = Sys.getenv(\"ACCESS_KEY_ID\"),\n        secret_access_key = Sys.getenv(\"SECRET_ACCESS_KEY\")\n      ),\n      profile = Sys.getenv(\"PROFILE\")\n    ),\n    region = Sys.getenv(\"REGION\")\n  ))\n}\n```\n:::\n\n\n\nUsers experienced with `DynamoDB` might notice that we don't use all config options,\nincluding `session_token` and `endpoint`. So far we haven't needed these. As mentioned before,\nthe `paws` documentation is extensive, so we point the user to `?paws::dynamodb` for more details\nregarding the configuration.\n\nOnce we have the client ready, we can create a table from `R`. We'll set both a `partition key`, \nor `HASH` key as specified in the `KeySchema` (`id`) and a `sort key` or `RANGE` (timestamp). Later\nwe can query the table using these keys which together form a composite primary key, but for now \nwe need to have the table, and start populating it.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncon <- dynamo_connect()\ncon$create_table(\n  AttributeDefinitions = list(\n    list(\n      AttributeName = \"id\",\n      AttributeType = \"N\"\n    ),\n    list(\n      AttributeName = \"timestamp\",\n      AttributeType = \"N\"\n    )\n  ),\n  KeySchema = list(\n    list(\n      AttributeName = \"id\",\n      KeyType = \"HASH\"\n    ),\n    list(\n      AttributeName = \"timestamp\",\n      KeyType = \"RANGE\"\n    )\n  ),\n  ProvisionedThroughput = list(\n    ReadCapacityUnits = 1,\n    WriteCapacityUnits = 1\n  ),\n  TableName = \"example\"\n)\n```\n:::\n\n\n\nTo confirm, we can query the DB for the list of tables:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncon$list_tables()\n```\n:::\n\n\n\n## Loading a large-ish table into DynamoDB\n\nOur next step is to start loading data. For this, we'll generate some random data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_to_put <- data.frame(\n  id = 1:10000,\n  timestamp = as.numeric(seq.POSIXt(\n    from = as.POSIXct(\"2020-01-01\"),\n    to = Sys.time(),\n    length.out = 10000\n  )),\n  measurement = sample.int(n = 100, size = 10000, replace = TRUE)\n)\n```\n:::\n\n\n\nWe can now try to put these data into our newly created table using the functions from\nour previous post on [`{paws}`](https://discindo.org/post/2022-04-30-using-r-and-paws-to-populate-dynamodb-tables/).\nBut that would not be a great approach, because there we are using APIs intended for putting single\nitems or a small volume. Instead, lets write a wrapper around the `batch_write_item` API, so we can load our data in bulk.\n\nTo batch write to our DynamoDB table, we need to create a requests for each item\n(row of the table) we wish to put. The put requests have the following format,\nidentical to the nested named list produced by our function `dynamo_item_prep`, but\nwith two more layers to label the `Item` and the type of request as `PutRequest`. \n\n```\nlist(PutRequest = list(Item = dynamo_item_prep(.item = data_to_put[1, ])))\n#> $PutRequest\n#> $PutRequest$Item\n#> $PutRequest$Item$id\n#> $PutRequest$Item$id$N\n#> [1] 1\n\n\n#> $PutRequest$Item$timestamp\n#> $PutRequest$Item$timestamp$N\n#> [1] 1577858400\n\n\n#> $PutRequest$Item$measurement\n#> $PutRequest$Item$measurement$N\n#> [1] 28\n```\n\nA list of 25 `PutRequest`s (or `DeleteRequest`s) weighing up to 16MB can be submitted in one batch\n(see `?paws.database::dynamodb_batch_write_item`), so we also need a way to divide our\n10K items into 25-item chunks. Our updated `dynamo_bulk_put` function applies these two \nmodifications to adjust the formatting and split the items into digestible chunks. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndynamo_bulk_put <- function(.con, .table, .df) {\n  requests <- lapply(1:nrow(.df), function(i) {\n    list(PutRequest = list(Item = dynamo_item_prep(.item = .df[i, ])))\n  })\n  \n  n_items <- length(requests)\n  # from https://stackoverflow.com/a/18857482/8543257\n  chunked <-\n    split(requests, rep(1:ceiling(n_items / 25), each = 25)[1:n_items])\n  \n  lapply(chunked, function(L) {\n    requestList <- list()\n    requestList[[.table]] <- L\n    .con$batch_write_item(RequestItems = requestList)\n  })\n}\n```\n:::\n\n\n\nTo test this, we first delete and re-create the `example` table:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncon$delete_table(TableName = \"example\")\ncon$create_table(\n  AttributeDefinitions = list(\n    list(\n      AttributeName = \"id\",\n      AttributeType = \"N\"\n    ),\n    list(\n      AttributeName = \"timestamp\",\n      AttributeType = \"N\"\n    )\n  ),\n  KeySchema = list(\n    list(\n      AttributeName = \"id\",\n      KeyType = \"HASH\"\n    ),\n    list(\n      AttributeName = \"timestamp\",\n      KeyType = \"RANGE\"\n    )\n  ),\n  ProvisionedThroughput = list(\n    ReadCapacityUnits = 1,\n    WriteCapacityUnits = 1\n  ),\n  TableName = \"example\"\n)\n```\n:::\n\n\n\nAnd then try to put all 10K rows:\n\n```\ndynamo_bulk_put(.con = con, .table = \"example\", .df = data_to_put)\n#> Error: com.amazonaws.dynamodb.v20120810 (HTTP 400). The level of configured provisioned throughput for the table was\n#> exceeded. Consider increasing your provisioning level with the UpdateTable API.\n```\n\nBut we get an error because we are making more requests than the default provisioned throughput limit for our table. \n\nTo address this, we should increase the provisioning level using `dynamodb_update_table`. In this\ncase we should increase the `WriteCapacityUnits`, as we are trying to write data,\nand try to load our 10K rows again. For details on the meaning of the capacity units,\nand different types of provisioning for DynamoDB tables, consult the [official documentation](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncon$update_table(\n  ProvisionedThroughput = list(\n    ReadCapacityUnits = 1,\n    WriteCapacityUnits = 50\n  ),\n  TableName = \"example\"\n)\ndynamo_bulk_put(.con = con, .table = \"example\", .df = data_to_put)\n```\n:::\n\n\n\nAfter loading all of our data, we can check the number of items in the cloud table with:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncon$scan(TableName = \"example\", Select = \"COUNT\")$Count\n#> [1] 10000\n```\n:::\n\n\n\nFinally, remember to update the provisioning to lower the write capacity units to default level,\nand avoid paying four resources we no longer need.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncon$update_table(\n  ProvisionedThroughput = list(\n    ReadCapacityUnits = 1,\n    WriteCapacityUnits = 1\n  ),\n  TableName = \"example\"\n)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}