{
  "hash": "3d772a4c56ed4b56c203556d67788a2c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Jumping in the Ducklake with nothing but R on\nauthor: \n  - name: novica\n    url: \"https://discindo.org/authors/novica\"\ndate: '2025-09-20'\ncategories:\n  - R \n  - ducklake\n  - duckdb\ndescription: \"Navigating the strange world of lakes, table formats, and catalogs from R\"\nexecute:\n  eval: false\nimage: \"images/image.png\"\n---\n\n\n\n[![](https://ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/J3J8133RYV)\n\nA week or so ago I was listening to a podcast about [Ducklake](https://ducklake.select/), the new table format by DuckDB, and started thinking if it can be used from R, specifically to mimic what would one do on cloud providers such as Databricks.\n\n`dplyr` and friends are mostly about cleaning and summarizing data, and the [lake(house)](https://www.databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html) idea is mostly about the so called 'medallion architecture' where data moves from raw and messy, to clean and validated, to business ready. So it's kind of like:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_raw |> \n  mutate(something) |> \n  group__by(something) |> \n  summarise(something)\n```\n:::\n\n\n\nBut why, you may ask? Well I think most data is not big data, but all data goes through this same process of cleaning and summarizing, so why not try and use some of the nice ideas to add to an R workflow.\n\nNote: If you are thinking about following through this post this is a good time to do `install.packages('duckdb')` if there is not a [binary distribution](https://r.duckdb.org/#installation-from-cran) for your system. It takes a while to compile it.\n\nOK. This is what we will try to do next: \n1. Set up a Ducklake; \n2. Write some data to it; \n3. Visualize relationships among tables.\n\n## Setting up a Ducklake\n\nAssuming you have opened a new `Rstudio` project with `git` enabled, and the `duckdb` package is installed on your system setting up Ducklake is as follows:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"duckdb\")\n\ncon <- dbConnect(duckdb(), dbdir=\":memory:\")\n\ndbExecute(con, \"INSTALL ducklake;\")\n\ndbExecute(con, \"ATTACH 'ducklake:metadata.ducklake' AS r_ducklake;\")\n\ndbExecute(con, \"USE r_ducklake;\")\n```\n:::\n\n\n\nThis creates a in memory `duckdb` database, installs the `ducklake` extension and then creates the `ducklake` on disk. In your project you will see two files: `metadata.ducklake` and `metadata.ducklake.wal`. The first one is a `duckdb` database with a different extension and the second is the `write-ahead log` that `duckdb` uses to manage the metadata.\n\nIf we close this connection (but if you do this remember to run the above again before continuing below:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbDisconnect(con)\n```\n:::\n\n\n\nAnd then open the `ducklake` file with a database browser, for example the `duckdb` cli:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n$ duckdb -ui metadata.ducklake \n```\n:::\n\n\n\nyou will see that it is a database with the tables as described on the [specification overview](https://ducklake.select/docs/stable/specification/tables/overview.html) for the table format.\n\n## Writing data to the ducklake\n\nNext I will pretend that the `mtcars` dataset needs to be cleaned and aggregated. There may be a better datasets to give this example, but it is simple to see the steps like this. I am also using `SQL` but same can be achieved with `dplyr` or anything handy in R:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bronze: raw data\ndbWriteTable(con, \"bronze_mtcars\", mtcars)\n\n# Silver: filtered / cleaned\ndbExecute(con, \"\n  CREATE TABLE silver_mtcars AS\n  SELECT *, (mpg/mean(mpg) OVER()) AS mpg_norm\n  FROM bronze_mtcars;\n\")\n\n# Gold: aggregated summary\ndbExecute(con, \"\n  CREATE TABLE gold_mtcars AS\n  SELECT cyl, AVG(mpg) AS avg_mpg, AVG(mpg_norm) AS avg_mpg_norm\n  FROM silver_mtcars\n  GROUP BY cyl;\n\")\n```\n:::\n\n\n\nIn the medallion architecture the first layer, the raw data, is called bronze, and in this case we just write the `mtcars` as is.\n\nThen the second layer, the silver layer, we sort of clean these data. I have no idea if normalizing miles per gallon makes sense, but hey why not.\n\nThe final layer is about an aggregate summary, a gold layer, so group by and summarize is what is happening there.\n\nIf you run the above in your demo project you will notice new files created:\n\n```         \n[metadata.ducklake.files]$ tree\n.\n└── main\n    ├── bronze_mtcars\n    │   └── ducklake-01996884-c393-7134-bee0-cbffb31e19c7.parquet\n    ├── gold_mtcars\n    │   └── ducklake-01996884-c3cd-7ae6-bcaa-0b69196040cb.parquet\n    └── silver_mtcars\n        └── ducklake-01996884-c3b5-7d9d-b9f8-05e3c8377bee.parquet\n```\n\nThis is ducklake keeping track of your data in `parquet` format.\n\nNext we need to make something to track the relationships between the tables. For now i think this is not supported in ducklake(v.0.0.3), but probably will be in the future.\n\nIdeally the following should be executed as each table is written, but for sake of example it is below:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbExecute(con, \"\nCREATE TABLE IF NOT EXISTS ducklake_lineage (\n  parent_table VARCHAR,\n  child_table  VARCHAR,\n  created_at   TIMESTAMP,\n  description  VARCHAR\n);\n\")\n\n# After creating a silver table from bronze\ndbExecute(con, \"\n    INSERT INTO ducklake_lineage (parent_table, child_table, created_at, description)\n    VALUES ('bronze_mtcars', 'silver_mtcars', NOW(), 'normalization / filtering');\n\")\n\n# After creating gold table from silver\ndbExecute(con, \"\n    INSERT INTO ducklake_lineage (parent_table, child_table, created_at, description)\n    VALUES ('silver_mtcars', 'gold_mtcars', NOW(), 'aggregation');\n\")\n```\n:::\n\n\n\nLet's imagine that we have done some more transformation and maybe created 2 more silver tables and one more gold table. Of course, the `decription` should be updated to make more sense:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbExecute(con, \"\n    INSERT INTO ducklake_lineage (parent_table, child_table, created_at, description)\n    VALUES ('bronze_mtcars', 'silver_mtcars_2', NOW(), 'normalization / filtering');\n\")\n\ndbExecute(con, \"\n    INSERT INTO ducklake_lineage (parent_table, child_table, created_at, description)\n    VALUES ('bronze_mtcars', 'silver_mtcars_3', NOW(), 'normalization / filtering');\n\")\n\ndbExecute(con, \"\n    INSERT INTO ducklake_lineage (parent_table, child_table, created_at, description)\n    VALUES ('silver_mtcars', 'gold_mtcars_2', NOW(), 'aggregation');\n\")\n```\n:::\n\n\n\n## Visualize relationships among tables\n\nSince digging trough the parquet files to see what the relationships between the tables are does not make much sense, we can use some of good old `ggplot2` and friend magic to visualize that.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(igraph)\nlibrary(ggraph)\n\nedges <- dbGetQuery(con, \"SELECT parent_table AS from_table, child_table AS to_table FROM ducklake_lineage\")\ng <- graph_from_data_frame(edges, directed = TRUE)\n\nggraph(g, layout = \"tree\") +\n  geom_edge_diagonal(arrow = arrow(length = unit(4, 'mm')), end_cap = circle(3, 'mm')) +\n  geom_node_point(shape = 15, size = 8, color = \"steelblue\", fill = \"lightblue\") + \n  geom_node_text(aes(label = name), vjust = -1.2, size = 4) +               \n  coord_flip() +\n  scale_y_reverse() +  \n  theme_void() +       \n  ggtitle(\"DuckLake Medallion Lineage\")\n```\n:::\n\n\n\n![](images/plot.png)\n\nThis is the sort of a thing you get to see under the lineage in Databricks' [Unity Catalog](https://docs.databricks.com/aws/en/data-governance/unity-catalog/data-lineage), although not as elaborate. However, a proper `ggplot2` wizard may succeed in making duplicating that view.  \n\nYou can imagine having this plot as part of a Quarto report or a Shiny app with some interactivity to keep track of the transformations that happen to the data you are managing.\n\n\n## Summary\n\nAs we set out in the beginning we set up a Ducklake with R, wrote some data to it and visualized relationships among tables. While the example is trivial, I think it is obvious that this could be a useful approach for managing local pipelines and for prototyping things that could run on the cloud. \n\nFor local pipelines in particular,  I can see this approach been used with something like [{maestro}](https://github.com/whipson/maestro) to orchestrate a data pipeline. \n\nThe code above is available as a [gist](https://gist.github.com/novica/ff55fc4d79c3007bb1acba40e74e9d25).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}